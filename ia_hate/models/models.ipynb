{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de3fe4a-8871-4a23-af11-059bfd820b0a",
   "metadata": {},
   "source": [
    "### Modelos a entrenar\n",
    "\n",
    "1. M√°quinas de soporte vectorial SVM\n",
    "2. Bosques Aleatorios RF\n",
    "3. Regresi√≥n Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2557988b-c059-484a-8564-354e969a35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics \\\n",
    "import classification_report, recall_score, accuracy_score,precision_score, make_scorer,confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle\n",
    "\n",
    "def myConfMatrix(confusion_array,labels,columns):\n",
    "    df = pd.DataFrame(confusion_array, index=labels, columns=columns)\n",
    "    return df\n",
    "\n",
    "def balanceDF(df,labels_dict):\n",
    "    violent = df.loc[df[\"label\"] == labels_dict[\"VIOLENT\"]]\n",
    "    nonviolent = df.loc[df[\"label\"] == labels_dict[\"NONVIOLENT\"]]\n",
    "    violent_patched = violent.sample(nonviolent.shape[0],random_state=0)\n",
    "    #balanced\n",
    "    bdf = pd.concat([violent_patched,nonviolent])\n",
    "    return bdf\n",
    "\n",
    "\n",
    "cm_labels = ['violent', 'nonviolent']\n",
    "cm_columns = ['Predicted violent', 'predicted nonviolent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482a661-22cd-4c94-a768-9de8a9034026",
   "metadata": {},
   "source": [
    "* Map to Violent and Non-Violent tags. \n",
    "* Create total dataset and balanced dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "dffc7ddc-a2ab-45d6-a961-b38abeeca01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "labels_dict = {\n",
    "    \"VIOLENT\": \"violent\",\n",
    "    \"NONVIOLENT\":\"nonviolent\"\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"../master_data/data.csv\")\n",
    "# mapping label to 1 -> violent 0 -> non-violent\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: labels_dict[\"VIOLENT\"] if x == 1 else labels_dict[\"NONVIOLENT\"])\n",
    "df['feature'] = df['feature'].str.replace('\\xa0', ' ', regex=False)\n",
    "\n",
    "\n",
    "\n",
    "# df[\"word_count\"] = df[\"feature\"].str.split().str.len()\n",
    "# df[\"word_count\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b371c5c-8f91-4af0-8ca5-e9ce132a135e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be82fd14-5c23-4e5a-9da0-3428acdc46cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bcfed481-bf0d-487f-a2be-324d56f02962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>es sexy.</td>\n",
       "      <td>violent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eres m√≠a.</td>\n",
       "      <td>violent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>la gorda.</td>\n",
       "      <td>violent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a fregar.</td>\n",
       "      <td>violent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca- gona.</td>\n",
       "      <td>violent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature    label  tweet_id origin\n",
       "0   es sexy.  violent       NaN    rnn\n",
       "1  eres m√≠a.  violent       NaN    rnn\n",
       "2  la gorda.  violent       NaN    rnn\n",
       "3  a fregar.  violent       NaN    rnn\n",
       "4  ca- gona.  violent       NaN    rnn"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4ba789-695d-4b88-a196-f422f3fb7e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11717, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d082501b-5b65-45b8-8f57-cf6a8e6bac72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "violent       8689\n",
       "nonviolent    3028\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e87ae17d-3773-4641-98b5-59c4959127bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "violent       3028\n",
       "nonviolent    3028\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdf.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4a67c-2990-41ac-8d8f-c0bbde8e0e81",
   "metadata": {},
   "source": [
    "Define pipelines, \n",
    "Note df is full dataset and bdf is balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cbdd0bc9-1704-4578-a7d8-c8c726e88c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_features = [\"feature\"]\n",
    "\n",
    "numeric_features = [\"length\", \"punct\"]\n",
    "\n",
    "# numberic featuers will be ignored as they are non-informative, as can be seen on EDA.\n",
    "\n",
    "    # numeric_transformer = Pipeline(\n",
    "    #     steps=[(\"scaler\", StandardScaler())]\n",
    "    # )\n",
    "    \n",
    "    # categorical_transformer = Pipeline(\n",
    "    #     steps=[\n",
    "    #         (\"squeeze\", FunctionTransformer(lambda x: x.squeeze(),validate=True)), # make sure you pass a series\n",
    "    #         (\"tfidf\",TfidfVectorizer())\n",
    "    #     ]\n",
    "    # )\n",
    "    \n",
    "    # preprocessor = ColumnTransformer(\n",
    "    #     transformers=[\n",
    "    #         # (\"num\", numeric_transformer, numeric_features),\n",
    "    #         (\"cat\", categorical_transformer, categorical_features)\n",
    "    #     ]\n",
    "    # )\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tfidf\", TfidfVectorizer(), \"feature\")  # Apply TfidfVectorizer to the 'feature' column\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifierRF = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\",preprocessor),\n",
    "        (\"rf\",RandomForestClassifier(n_estimators=100,random_state=0,criterion=\"gini\",class_weight=\"balanced\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifierSVC = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\",preprocessor),\n",
    "        (\"svc\",svm.SVC(random_state=0,class_weight=\"balanced\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifierLR = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\",preprocessor),\n",
    "        (\"logreg\", LogisticRegression(solver=\"liblinear\", random_state=0,max_iter=100,class_weight='balanced'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "param_grid_svc = {\n",
    "    \"svc__C\": [.1,1, 10, 100],\n",
    "    \"svc__kernel\": ['linear', 'rbf', 'sigmoid'],\n",
    "    \"svc__gamma\": [\"auto\",\"scale\"],\n",
    "    \"svc__class_weight\":[None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_rf = {\n",
    "    'rf__n_estimators': [100, 200, 500],\n",
    "    'rf__max_depth': [None, 10, 20, 30],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    'rf__max_features': ['sqrt', 'log2'],\n",
    "    'rf__bootstrap': [True, False],\n",
    "    'rf__criterion': ['gini', 'entropy'],\n",
    "    'rf__class_weight':[None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_logreg = {\n",
    "    'logreg__C': [0.01, 0.1, 1, 10],\n",
    "    'logreg__penalty': ['l1', 'l2'],\n",
    "    'logreg__solver': ['liblinear','saga'],\n",
    "    'logreg__class_weight':[None, 'balanced']\n",
    "}\n",
    "\n",
    "# please note that given the imblanace in the training data, we use clss weight balanced so we penalize more erverley the errors in the\n",
    "# minority class, so our classifier is not biased towards the majority class which is violent. \n",
    "\n",
    "\n",
    "\n",
    "# scorer = make_scorer(accuracy_score)\n",
    "scorer = make_scorer(precision_score,pos_label=labels_dict[\"VIOLENT\"],average=\"binary\",zero_division=0.0)\n",
    "\n",
    "# we optimize for precision because a false positive is more costly than a false negative. \n",
    "\n",
    "\n",
    "# best_svc = \\\n",
    "# GridSearchCV(classifierSVC, param_grid_svc,n_jobs=8,cv=5,\n",
    "#              scoring=scorer\n",
    "# )\n",
    "# best_rf = GridSearchCV(classifierRF, param_grid_rf,n_jobs=8,cv=5,\n",
    "#                        scoring=scorer\n",
    "#                         )\n",
    "# best_logreg = GridSearchCV(classifierLR, param_grid_logreg,n_jobs=8,cv=5,scoring=scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5fe02-2db0-44cb-9149-59e8c00f98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition = np.logical_or(bdf[\"feature\"].str.contains(\"robamaridos\"), bdf[\"feature\"].str.contains(\"ella tiene que pedirle permiso\"))\n",
    "\n",
    "\n",
    "# corpus = bdf.loc[condition,[\"feature\"]].squeeze()\n",
    "# print(corpus.head())\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(corpus)\n",
    "# print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# print(X.shape)\n",
    "# X.toarray()\n",
    "\n",
    "# transformed_data = preprocessor.fit_transform(X_train)\n",
    "# transformed_data.toarray()[188,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dafa42a5-51f4-4645-be45-8b250aabff1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "violent       3028\n",
       "nonviolent    3028\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdf.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4a160cf2-05ec-4f54-82f1-db37c6ac0c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11717, 4)\n",
      "label\n",
      "violent       2603\n",
      "nonviolent     913\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "violent       2603\n",
      "nonviolent     913\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "violent       913\n",
      "nonviolent    913\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "data = df\n",
    "X_train, X_test_full, y_train, y_test_full =\\\n",
    "train_test_split(data.loc[:,[\"feature\"]],data[\"label\"], test_size= 0.3, random_state= 1)\n",
    "print(df.shape)\n",
    "print(y_test_full.value_counts())\n",
    "\n",
    "# --- join X and y for resampling ---\n",
    "test_df = X_test_full.copy()\n",
    "test_df[\"label\"] = y_test_full.values\n",
    "\n",
    "# --- separate classes ---\n",
    "violent_df     = test_df[test_df[\"label\"] == \"violent\"]\n",
    "nonviolent_df  = test_df[test_df[\"label\"] == \"nonviolent\"]\n",
    "\n",
    "# target size = minority count (here nonviolent is smaller: 913)\n",
    "target_n = 913\n",
    "\n",
    "# --- downsample majority to match minority ---\n",
    "violent_down = resample(violent_df,     replace=False, n_samples=target_n, random_state=1)\n",
    "nonviolent_k = resample(nonviolent_df,  replace=False, n_samples=target_n, random_state=1)\n",
    "\n",
    "balanced_test = pd.concat([violent_down, nonviolent_k]).sample(frac=1, random_state=1)  # shuffle\n",
    "\n",
    "# --- split back to X, y ---\n",
    "X_test = balanced_test[\"feature\"]\n",
    "y_test = balanced_test[\"label\"]\n",
    "\n",
    "print(y_test_full.value_counts())  # original imbalanced test\n",
    "print(y_test.value_counts())   # now balanced 1:1\n",
    "\n",
    "\n",
    "#### why do we do this ? \n",
    "\n",
    "### IMPORTANT EXPLANATION\n",
    "\n",
    "# our test dataset as is is imbalanced 70-30, (violent-non violent respectively)\n",
    "# there is nothing in research to suggest that the natural priors resemble these prior distributions.\n",
    "# Hence validating models on an imbalanced violent dataset would artificially boost precision for identifying violent texts.\n",
    "# It is safer to validate detectors on a balanced dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be1df42-1e9f-4e26-b0cf-f8f7ce255f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0145e-6fcd-4c5c-8393-2123975391c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c779cfd-e0e9-43bf-ad22-e4a9ec34762d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6056, 4)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "feca3bae-7550-4091-88d9-00969c90a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = {\n",
    "    'svc':(classifierSVC,param_grid_svc),\n",
    "    'rf': (classifierRF,param_grid_rf),\n",
    "    'logreg':(classifierLR, param_grid_logreg)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "74c5352d-f774-40bb-b743-a1aa1f2b1599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trained_models = {}\n",
    "\n",
    "for model_name,model_logic in models_to_train.items():\n",
    "    classifier, param_grid = model_logic\n",
    "    best_model = GridSearchCV(classifier, param_grid,n_jobs=8,cv=5,scoring=scorer)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    trained_models[model_name] = best_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c97790bd-0897-4fc8-ac09-8c25e64116c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# for model_name,model in trained_models.items():\n",
    "#     filename = f'{model_name}_all2.pkl'\n",
    "#     with open(filename, 'wb') as file:\n",
    "#         pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb2b6746-4dce-4215-bb45-c6504a185704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "import pickle\n",
    "# for model_name,model in trained_models.items():\n",
    "#     filename = f'{model_name}_bdf.pkl'\n",
    "#     with open(filename, 'wb') as file:\n",
    "#         pickle.dump(model, file)\n",
    "\n",
    "# # Load the model\n",
    "# with open(filename, 'rb') as file:\n",
    "#     loaded_model = pickle.load(file)\n",
    "\n",
    "# # Use the loaded model\n",
    "# predictions = loaded_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "78f19017-2f0e-4f79-823b-a1544de48a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2401</th>\n",
       "      <td>Pregunta feminista seria: si una mujer es mas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>Uno deber√≠a estar siempre enamorado. Por eso j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4545</th>\n",
       "      <td>Dios es para los hombres y la religi√≥n para la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>Las mujeres escuchen en silencio las instrucci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6785</th>\n",
       "      <td>el cuerpo de la mujer y el calz√≥n , y maldita ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11571</th>\n",
       "      <td>üî¥#ACTUALIZACI√ìN Greta Thunberg se presenta por...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3705</th>\n",
       "      <td>La ciencia se compone de errores, que a su vez...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4825</th>\n",
       "      <td>En los inicios de un amor los amantes hablan d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6400</th>\n",
       "      <td>que puta.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>La mayor√≠a de las ideas fundamentales de la ci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1826 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 feature\n",
       "2401   Pregunta feminista seria: si una mujer es mas ...\n",
       "4236   Uno deber√≠a estar siempre enamorado. Por eso j...\n",
       "4545   Dios es para los hombres y la religi√≥n para la...\n",
       "1885   Las mujeres escuchen en silencio las instrucci...\n",
       "6785   el cuerpo de la mujer y el calz√≥n , y maldita ...\n",
       "...                                                  ...\n",
       "11571  üî¥#ACTUALIZACI√ìN Greta Thunberg se presenta por...\n",
       "3705   La ciencia se compone de errores, que a su vez...\n",
       "4825   En los inicios de un amor los amantes hablan d...\n",
       "6400                                           que puta.\n",
       "4631   La mayor√≠a de las ideas fundamentales de la ci...\n",
       "\n",
       "[1826 rows x 1 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "24602a1e-acb3-4fcb-9454-3cbb5fdc08b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######MODEL: svc\n",
      "Best parameters to evaluate on (CV score=0.946):\n",
      "Best params were :{'svc__C': 1, 'svc__class_weight': 'balanced', 'svc__gamma': 'scale', 'svc__kernel': 'sigmoid'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.90      0.88      0.89       913\n",
      "     violent       0.88      0.91      0.89       913\n",
      "\n",
      "    accuracy                           0.89      1826\n",
      "   macro avg       0.89      0.89      0.89      1826\n",
      "weighted avg       0.89      0.89      0.89      1826\n",
      "\n",
      "#######MODEL: rf\n",
      "Best parameters to evaluate on (CV score=0.973):\n",
      "Best params were :{'rf__bootstrap': False, 'rf__class_weight': 'balanced', 'rf__criterion': 'entropy', 'rf__max_depth': 10, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 4, 'rf__min_samples_split': 10, 'rf__n_estimators': 500}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.76      0.96      0.84       913\n",
      "     violent       0.94      0.69      0.80       913\n",
      "\n",
      "    accuracy                           0.82      1826\n",
      "   macro avg       0.85      0.82      0.82      1826\n",
      "weighted avg       0.85      0.82      0.82      1826\n",
      "\n",
      "#######MODEL: logreg\n",
      "Best parameters to evaluate on (CV score=0.961):\n",
      "Best params were :{'logreg__C': 1, 'logreg__class_weight': 'balanced', 'logreg__penalty': 'l1', 'logreg__solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.84      0.92      0.88       913\n",
      "     violent       0.91      0.82      0.86       913\n",
      "\n",
      "    accuracy                           0.87      1826\n",
      "   macro avg       0.87      0.87      0.87      1826\n",
      "weighted avg       0.87      0.87      0.87      1826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"all2\"\n",
    "\n",
    "for model_name,_ in models_to_train.items():\n",
    "    print(f\"#######MODEL: {model_name}\")\n",
    "    with open(model_name+f\"_{dataset_type}.pkl\", 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    print(f\"Best parameters to evaluate on (CV score={model.best_score_:.3f}):\")\n",
    "    print(f\"Best params were :{model.best_params_}\")\n",
    "    y_pred_model = model.predict(X_test.to_frame())\n",
    "    y_pred_model_train = model.predict(X_train)\n",
    "    report_model_train = classification_report(y_train, y_pred_model_train)\n",
    "    # print(report_model_train)\n",
    "    report_model = classification_report(y_test, y_pred_model)\n",
    "    print(report_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cab17f-8f60-4d1f-b43a-584818cc0dee",
   "metadata": {},
   "source": [
    "#### The best model is the SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc4824-c0ff-43b3-aa63-19ef557b0d49",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a5e8baac-23ff-4ae1-bbf7-26f907361d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.9.16 environment at: /Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m32 packages\u001b[0m \u001b[2min 4.25s\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m12 packages\u001b[0m \u001b[2min 52.39s\u001b[0m\u001b[0m                                           \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m14 packages\u001b[0m \u001b[2min 211ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.19.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.11.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentence-transformers\u001b[0m\u001b[2m==5.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install gensim sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b84248-fc87-4fa9-add6-c963420874d8",
   "metadata": {},
   "source": [
    "Lets start with static embeddings, which is embeddings for words and then we will mean the pero word embeddings for each sentence. The mean reduction is quite important althoug simple it does provide a good semantic summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b4edec86-e312-4bea-bb9c-525e1c7a1473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-19 01:44:09--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.bin.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 2600:9000:289d:2c00:13:6e38:acc0:93a1, 2600:9000:289d:5600:13:6e38:acc0:93a1, 2600:9000:289d:7600:13:6e38:acc0:93a1, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|2600:9000:289d:2c00:13:6e38:acc0:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4500107671 (4.2G) [application/octet-stream]\n",
      "Saving to: ‚Äòcc.es.300.bin.gz‚Äô\n",
      "\n",
      "cc.es.300.bin.gz    100%[===================>]   4.19G  2.09MB/s    in 29m 3s  \n",
      "\n",
      "2025-11-19 02:13:13 (2.46 MB/s) - ‚Äòcc.es.300.bin.gz‚Äô saved [4500107671/4500107671]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "01b59c0f-6c45-4be8-a300-e26f62756d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import load_facebook_vectors \n",
    "\n",
    "ft = load_facebook_vectors(\"cc.es.300.bin\") \n",
    "\n",
    "def sentence_vec(s): \n",
    "    toks = s.lower().split() \n",
    "    vecs = [ft[w] for w in toks if w in ft.key_to_index] \n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(ft.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b16d5513-c612-4145-8aed-40891a86cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                          | 1/8201 [00:00<00:01, 4275.54it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1826/1826 [00:00<00:00, 32548.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix shape: (1, 300)\n",
      "Test matrix shape: (1826, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm  # optional progress bar\n",
    "\n",
    "# Assuming your data:\n",
    "# X_train, y_train, X_test, y_test\n",
    "# Each X_* contains the text sentences (str)\n",
    "\n",
    "# 1. Compute sentence vectors\n",
    "def build_fasttext_matrix(texts):\n",
    "    return np.vstack([sentence_vec(s) for s in tqdm(texts)])\n",
    "\n",
    "# 2. Transform both train and test sets\n",
    "X_train_vec = build_fasttext_matrix(X_train)\n",
    "X_test_vec  = build_fasttext_matrix(X_test)\n",
    "\n",
    "print(\"Train matrix shape:\", X_train_vec.shape)\n",
    "print(\"Test matrix shape:\", X_test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bcfb8be4-27ab-4c69-b13e-b34da9ca3a2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 8201]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m      4\u001b[0m clf \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test_vec)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred, digits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[0;32m~/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1222\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1220\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m-> 1222\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1231\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[0;32m~/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:1389\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1370\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1371\u001b[0m     X,\n\u001b[1;32m   1372\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1384\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1385\u001b[0m )\n\u001b[1;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m-> 1389\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    478\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 8201]"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42)\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e51ad0c-4c56-4bc8-bdcc-859904ab3f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "train_test_split(data.loc[:,[\"feature\"]],data[\"label\"], test_size= 0.3, random_state= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50eb9713-1882-4a85-a201-46b6fa8d40a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_svc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# best_svc.fit(X_train, y_train)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing precission as the Best parameter to evaluate on (CV score=\u001b[39m\u001b[38;5;132;01m%0.3f\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[43mbest_svc\u001b[49m\u001b[38;5;241m.\u001b[39mbest_score_)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_svc\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m      4\u001b[0m y_pred_svc \u001b[38;5;241m=\u001b[39m best_svc\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_svc' is not defined"
     ]
    }
   ],
   "source": [
    "# best_svc.fit(X_train, y_train)\n",
    "print(\"using precission as the Best parameter to evaluate on (CV score=%0.3f):\" % best_svc.best_score_)\n",
    "print(best_svc.best_params_)\n",
    "y_pred_svc = best_svc.predict(X_test)\n",
    "y_pred_svc_train = best_svc.predict(X_train)\n",
    "report_svc_train = classification_report(y_train, y_pred_svc_train)\n",
    "print(report_svc_train)\n",
    "report_svc = classification_report(y_test, y_pred_svc)\n",
    "print(report_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272c921-6498-45c7-b6d2-589512fd5d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define row and column labels\n",
    "\n",
    "myConfMatrix(confusion_matrix(y_test, y_pred_svc,labels=cm_labels),cm_labels,cm_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfe192-f367-4913-99c8-3caf0cde1ef3",
   "metadata": {},
   "source": [
    "#### Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ba988-c2b7-4cec-80d3-074187e4183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = bdf\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "train_test_split(data.loc[:,[\"feature\"]],data[\"label\"], test_size= 0.3, random_state= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356193df-a562-4da3-be00-e5876810a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svc.fit(X_train, y_train)\n",
    "print(\"using precission as the Best parameter to evaluate on (CV score=%0.3f):\" % best_svc.best_score_)\n",
    "print(best_svc.best_params_)\n",
    "y_pred_svc = best_svc.predict(X_test)\n",
    "y_pred_svc_train = best_svc.predict(X_train)\n",
    "report_svc_train = classification_report(y_train, y_pred_svc_train)\n",
    "print(report_svc_train)\n",
    "report_svc = classification_report(y_test, y_pred_svc)\n",
    "print(report_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e8a669-3726-4e43-9481-bdacca88117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d8591b-5b94-4893-9800-11f8bf11308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred_svc,labels=cm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e523b-5a03-4258-aeac-b86f106a9ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define row and column labels\n",
    "\n",
    "myConfMatrix(confusion_matrix(y_test, y_pred_svc,labels=cm_labels),cm_labels,cm_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a742d9-b80b-40f1-bb0d-0d438fa3e8cb",
   "metadata": {},
   "source": [
    "#### Comments on balanced vs full:\n",
    "\n",
    "Precision is higher on balanced dataset vs full. .97 > .93, but the loss in recall is gigantic. \n",
    ".20 < .94.  \n",
    "\n",
    "The SVC trained on full data seems to loose a lot of information when balanced, maybe due to the fact that it is being trained on roughly half the data when balanced, and that causes information loss. \n",
    "\n",
    "balanced = 6056 observations\n",
    "full = 11700 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c70bb5e-242a-4603-9538-42ae7eb6b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493a8d60-1149-40ba-9f48-fbd7e7cb9ef3",
   "metadata": {},
   "source": [
    "## RF\n",
    "\n",
    "Results on Full and Balanced Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40dd6e9-c772-42c3-8487-a7e298fc5b4a",
   "metadata": {},
   "source": [
    "#### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a8cf7-96bd-4da2-b5c4-92b10cb1e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "train_test_split(data.loc[:,[\"feature\"]],data[\"label\"], test_size= 0.3, random_state= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab863b0-2232-48df-bf6e-044172f036a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf.fit(X_train,y_train)\n",
    "print(\"using precission as the Best parameter to evaluate on (CV score=%0.3f):\" % best_rf.best_score_)\n",
    "pprint(best_rf.best_params_)\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "y_pred_rf_train = best_rf.predict(X_train)\n",
    "report_rf_train = classification_report(y_train, y_pred_rf_train)\n",
    "print(report_rf_train)\n",
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "print(report_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6945232-fb1e-4064-912a-21860a6517a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "myConfMatrix(confusion_matrix(y_test, y_pred_rf,labels=labels),labels,columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc8d1ab-1993-4cd6-a891-686d3962ecc2",
   "metadata": {},
   "source": [
    "#### Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e43064-fad4-4564-9c3a-fb4437ebc4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =\\\n",
    "train_test_split(bdf.loc[:,[\"feature\"]],bdf[\"label\"], test_size= 0.3, random_state= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0249192-ea32-483c-885d-06d605c1845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf.fit(X_train,y_train)\n",
    "print(\"using precission as the Best parameter to evaluate on (CV score=%0.3f):\" % best_rf.best_score_)\n",
    "pprint(best_rf.best_params_)\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "y_pred_rf_train = best_rf.predict(X_train)\n",
    "report_rf_train = classification_report(y_train, y_pred_rf_train)\n",
    "print(report_rf_train)\n",
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "print(report_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3920bf65-e65f-4eb5-acfe-cefa4aac028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "myConfMatrix(confusion_matrix(y_test, y_pred_rf,labels=labels),labels,columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e22b1a-74ad-4138-b3cc-e769efb1deb4",
   "metadata": {},
   "source": [
    "precision .94, recall is .73 not so good with false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cdab28-2d5f-4c9d-9083-873f3948f6e4",
   "metadata": {},
   "source": [
    "Comment RF Balanced vs Full: \n",
    "\n",
    "comparing balanced to full, precision .94 > .90, recall .73 < .97.  The behavior is consistent with what we observed with SVM.\n",
    "marginal precision comes at a very steep recall cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ef7b5-7a64-477b-a475-477292090a02",
   "metadata": {},
   "source": [
    "Comment RF vs SVM: \n",
    "\n",
    "SVM is slightly better than RF in terms of precision.\n",
    "\n",
    "precision  SVM .93 >  RF. 90  recall SVM .94 < .97"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64312e75-d370-4c54-9d25-67287eda9d4e",
   "metadata": {},
   "source": [
    "Opinion for Giepetto\n",
    "\n",
    "Here's a breakdown of what might be happening with the SVM and Random Forest (RF) models across the imbalanced and balanced datasets, and guidance on what to prioritize given your use case:\n",
    "\n",
    "Key Observations\n",
    "SVM Results:\n",
    "\n",
    "Imbalanced dataset: Precision = 0.93, Recall = 0.94 (Good balance between precision and recall)\n",
    "\n",
    "Balanced dataset: Precision = 0.97, Recall = 0.20 (High precision but very low recall)\n",
    "\n",
    "Why?:\n",
    "\n",
    "With the balanced dataset, SVM is likely overfitting to the violent class because it now treats the violent and nonviolent classes equally, even though nonviolent is less common in the real-world distribution.\n",
    "This causes the model to become more conservative, predicting the violent label only when it's very confident, leading to high precision but poor recall.\n",
    "Random Forest Results:\n",
    "\n",
    "Imbalanced dataset: Precision = 0.90, Recall = 0.97 (Slight bias toward recall)\n",
    "\n",
    "Balanced dataset: Precision = 0.94, Recall = 0.73 (More balanced precision-recall tradeoff)\n",
    "\n",
    "Why?:\n",
    "\n",
    "RF handles imbalance better than SVM, especially with the imbalanced dataset, because it can inherently deal with class distributions using its splitting criteria (e.g., Gini or entropy).\n",
    "On the balanced dataset, RF becomes slightly more conservative, improving precision but reducing recall compared to the imbalanced dataset.\n",
    "What to Prioritize?\n",
    "Given that false positives (nonviolent classified as violent) are more critical than false negatives (violent classified as nonviolent), precision is more important in your case.\n",
    "\n",
    "However, you also need an acceptable level of recall to ensure the model isn't ignoring too many violent instances.\n",
    "\n",
    "Dataset Choice\n",
    "Imbalanced Dataset:\n",
    "\n",
    "Better represents the real-world distribution.\n",
    "Produces models with higher recall and acceptable precision (RF: 0.90/0.97; SVM: 0.93/0.94).\n",
    "Recommended if the goal is to generalize to real-world scenarios without resampling artifacts.\n",
    "Balanced Dataset:\n",
    "\n",
    "Useful if you want to emphasize precision (e.g., with SVM: 0.97 precision), but at the cost of significantly lower recall.\n",
    "May not represent the natural data distribution and could lead to suboptimal performance on real-world data.\n",
    "Model Choice\n",
    "Random Forest seems to be performing more consistently across both datasets:\n",
    "\n",
    "On the imbalanced dataset, RF achieves high recall (0.97) with decent precision (0.90).\n",
    "On the balanced dataset, RF strikes a better balance (0.94 precision, 0.73 recall) compared to SVM.\n",
    "RF's ability to handle imbalance natively makes it a robust choice, especially if you focus on imbalanced data.\n",
    "SVM, while achieving higher precision on the balanced dataset (0.97), suffers from extremely low recall (0.20), which may not be acceptable for detecting violent cases.\n",
    "\n",
    "Recommendation\n",
    "Use the imbalanced dataset with Random Forest:\n",
    "\n",
    "Precision = 0.90, Recall = 0.97\n",
    "This combination ensures you capture most violent cases (high recall) while keeping false positives (nonviolent misclassified as violent) relatively low.\n",
    "Consider threshold adjustment:\n",
    "\n",
    "For RF, adjust the decision threshold to further fine-tune the precision-recall tradeoff based on your specific requirements.\n",
    "Monitor real-world performance:\n",
    "\n",
    "Test the selected model on a held-out or real-world dataset to confirm it performs well on the expected distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab36e9-f002-416b-8017-b7474dcc3cc1",
   "metadata": {},
   "source": [
    "If you're uncertain about the real-world prevalence of hate speech, here are strategies to handle this ambiguity:\n",
    "\n",
    "1. Balanced Dataset for Training\n",
    "Using the balanced dataset (e.g., 3000 hate speech and 3000 non-hate speech) might help avoid biasing the model toward hate speech and result in a more conservative classifier.\n",
    "While this might slightly under-detect hate speech (lower recall), it aligns better with your goal of minimizing false positives.\n",
    "2. Imbalanced Dataset with Class Weighting\n",
    "Train on the full imbalanced dataset but apply class weighting to penalize misclassifications of non-hate speech more heavily.\n",
    "Example with Scikit-learn:\n",
    "python\n",
    "Copy code\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    class_weight={'violent': 1, 'nonviolent': 2},  # Adjust weights to penalize nonviolent misclassification more\n",
    "    random_state=42\n",
    ")\n",
    "3. Threshold Adjustment\n",
    "Even with an imbalanced dataset, you can adjust the decision threshold for the hate speech class to prioritize precision. This reduces the likelihood of false positives.\n",
    "4. External Validation\n",
    "Validate your model on a separate dataset (if available) that reflects a more typical distribution of hate vs. non-hate speech to ensure generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b5bf1-74f1-45ab-bb82-fbdeb04c8f16",
   "metadata": {},
   "source": [
    "Given the fact that the full dataset is very imbalanced. we will compare both best models on an identical test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd7c68-dbfb-49ce-89d9-55494e11af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid_rf = {\n",
    "#     \"rf__C\": [1, 10, 100],\n",
    "#     \"rf__kernel\": ['linear', 'rbf', 'sigmoid'],\n",
    "#     \"rf__gamma\": [\"auto\",\"scale\"]\n",
    "# }\n",
    "\n",
    "param_grid_rf = {\n",
    "    'rf__n_estimators': [100, 200, 500],\n",
    "    'rf__max_depth': [None, 10, 20, 30],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    # 'min_samples_leaf': [1, 2, 4],\n",
    "    # 'max_features': ['sqrt', 'log2'],\n",
    "    # 'bootstrap': [True, False],\n",
    "    # 'criterion': ['gini', 'entropy']\n",
    "}\n",
    "best_rf = GridSearchCV(classifierRF, param_grid_rf,n_jobs=8,cv=5,scoring=make_scorer(precision_score,pos_label=VIOLENT,average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a7bdb-e0da-46c6-8489-4db0d3e775fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c9466-291c-47ef-9ed3-eca58683299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf.fit(X_train,y_train)\n",
    "print(\"using precission as the Best parameter to evaluate on (CV score=%0.3f):\" % best_rf.best_score_)\n",
    "pprint(best_rf.best_params_)\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "y_pred_rf_train = best_rf.predict(X_train)\n",
    "report_rf_train = classification_report(y_train, y_pred_rf_train)\n",
    "print(report_rf_train)\n",
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "print(report_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c80dc-0db3-4c3e-84b0-60eca4f68bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneTest = pd.DataFrame(pd.Series([\n",
    "    \"Las mujeres pertenencen en la cocina, y la lavander√≠a\",\n",
    "    \"Casarse es la funci√≥n de una mujer\",\n",
    "    \"El amor es lo mas puro\",\n",
    "    \"texto lucha amor\"\n",
    "]),columns=[\"feature\"])\n",
    "best_svc.predict(oneTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d22896-605c-45e0-8496-a0bc38789176",
   "metadata": {},
   "source": [
    "Revisemos ahora el desempe√±o del modelo sin cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e31a34-1335-46d2-b959-5869bdb8ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"feature\"]\n",
    "\n",
    "# numeric_transformer = Pipeline(\n",
    "#     steps=[(\"scaler\", StandardScaler())]\n",
    "# )\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"squeez\", FunctionTransformer(lambda x: x.squeeze())), # make sure you pass a series\n",
    "        (\"tfidf\",TfidfVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifierSVC = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"svc\",svm.SVC(random_state=0))\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifierSVC2 = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\",TfidfVectorizer()),\n",
    "        (\"svc\",svm.SVC(random_state=0))\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d3b543-2f4a-4570-8f94-6787b4fc72b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c454e8c-f2f9-43c9-84ea-f767461a3988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73dc329-ac04-4602-be9b-0574dcbec0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_svc_simple = classifierSVC2.fit(X_train.squeeze(),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7e730-05b1-4a94-a48a-f3192852098d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34168552-3744-4d41-bb7b-db45604d518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mod_svc_simple.predict(X_test.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad46d8-38e5-4013-a271-29090373cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2085205d-74a8-45c3-ba51-18df3ed70b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "pprint(report[VIOLENT])\n",
    "pprint(report[NONVIOLENT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc0c65-95f0-4e15-a08c-f36c0398d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d2e61-bd4b-415f-9e6b-bc7636b4d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
