{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b09d198-80a3-4943-90f7-eea55956f0bc",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa7168b-71d5-4127-b080-18d954088eb0",
   "metadata": {},
   "source": [
    "## 1. Loading Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce86a5e-b5da-40d0-9136-b77fe4be3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics \\\n",
    "import classification_report, recall_score, accuracy_score,precision_score, make_scorer,confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle\n",
    "\n",
    "def myConfMatrix(confusion_array,labels,columns):\n",
    "    df = pd.DataFrame(confusion_array, index=labels, columns=columns)\n",
    "    return df\n",
    "\n",
    "def balanceDF(df):\n",
    "    violent = df.loc[df[\"label\"] == \"violent\"]\n",
    "    nonviolent = df.loc[df[\"label\"] == \"nonviolent\"]\n",
    "    violent_patched = violent.sample(nonviolent.shape[0],random_state=0)\n",
    "    #balanced\n",
    "    bdf = pd.concat([violent_patched,nonviolent])\n",
    "    return bdf\n",
    "\n",
    "### embeddings dependencies \n",
    "\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "ft = load_facebook_vectors(\"cc.es.300.bin\")\n",
    "\n",
    "_token_pat = re.compile(r\"[A-Za-zÃÃ‰ÃÃ“ÃšÃœÃ‘Ã¡Ã©Ã­Ã³ÃºÃ¼Ã±']+\")\n",
    "\n",
    "def _sentence_vec_fasttext(s, ft_model):\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if s is None else str(s)\n",
    "    toks = _token_pat.findall(s.lower())\n",
    "    vecs = [ft_model[w] for w in toks if w in ft_model.key_to_index]\n",
    "    return np.mean(vecs, axis=0).astype(np.float32) if vecs else np.zeros(ft_model.vector_size, dtype=np.float32)\n",
    "\n",
    "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ft_model, vector_size=None):\n",
    "        self.ft_model = ft_model\n",
    "        self.vector_size = vector_size  # optional override\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        texts = np.asarray(X).ravel()\n",
    "        vs = self.vector_size or self.ft_model.vector_size\n",
    "        # ensure consistent dtype/shape\n",
    "        out = np.zeros((len(texts), vs), dtype=np.float32)\n",
    "        for i, s in enumerate(texts):\n",
    "            out[i] = _sentence_vec_fasttext(s, self.ft_model)\n",
    "        return out\n",
    "\n",
    "###############\n",
    "\n",
    "\n",
    "VIOLENT_LABEL = \"violent\"\n",
    "NONVIOLENT_LABEL = \"nonviolent\"\n",
    "\n",
    "cm_labels = ['violent', 'nonviolent']\n",
    "cm_columns = ['Predicted violent', 'predicted nonviolent']\n",
    "\n",
    "\n",
    "\n",
    "categorical_features = [\"feature\"]\n",
    "\n",
    "# These were discarded as non informative. \n",
    "# numberic featuers will be ignored as they are non-informative, as can be seen on EDA.\n",
    "numeric_features = [\"length\", \"punct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54961e4f-54ff-481f-9ade-f825e586dd60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d8ce47-f666-4d17-a08a-5ff78e58ee87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba745748-892f-4e43-bd9f-cdf1d1b9f062",
   "metadata": {},
   "source": [
    "### 1.1 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ce1d9-d42b-4707-aef3-6a6f46e8611f",
   "metadata": {},
   "source": [
    "* Map to Violent and Non-Violent tags. \n",
    "* Create total dataset and balanced dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09949439-fddc-46cd-8976-f0b75c67fde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full db data:\n",
      "label\n",
      "violent       8689\n",
      "nonviolent    3028\n",
      "Name: count, dtype: int64\n",
      "balanced db data:\n",
      "label\n",
      "violent       8689\n",
      "nonviolent    3028\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../master_data/data.csv\")\n",
    "# mapping label to 1 -> violent 0 -> non-violent\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: VIOLENT_LABEL if x == 1 else NONVIOLENT_LABEL)\n",
    "df['feature'] = df['feature'].str.replace('\\xa0', ' ', regex=False)\n",
    "\n",
    "bdf = balanceDF(df)\n",
    "\n",
    "print(\"full db data:\")\n",
    "print(df.label.value_counts())\n",
    "print(\"balanced db data:\")\n",
    "print(df.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63dbb6-3a1d-4b9a-bc8d-52f016c4ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f414065-fa7d-4d36-984d-cdde7a1bb17d",
   "metadata": {},
   "source": [
    "### Split dataset into train and test. \n",
    "\n",
    "Note that we rebalance the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e22072-bca2-41dc-aab2-3295d098ce3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the entire dataset is (11717, 4)\n",
      "the entire test dataset:\n",
      "label\n",
      "violent       2603\n",
      "nonviolent     913\n",
      "Name: count, dtype: int64\n",
      "the resampled balanced test dataset:\n",
      "label\n",
      "violent       913\n",
      "nonviolent    913\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "data = df\n",
    "X_train, X_test_full, y_train, y_test_full =\\\n",
    "train_test_split(data.loc[:,[\"feature\"]],data[\"label\"], test_size= 0.3, random_state= 1)\n",
    "print(f\"the shape of the entire dataset is {df.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- join X and y for resampling ---\n",
    "test_df = X_test_full.copy()\n",
    "test_df[\"label\"] = y_test_full.values\n",
    "\n",
    "# --- separate classes ---\n",
    "violent_df     = test_df[test_df[\"label\"] == \"violent\"]\n",
    "nonviolent_df  = test_df[test_df[\"label\"] == \"nonviolent\"]\n",
    "\n",
    "# target size = minority count (here nonviolent is smaller: 913)\n",
    "target_n = 913\n",
    "\n",
    "# --- downsample majority to match minority ---\n",
    "violent_down = resample(violent_df,     replace=False, n_samples=target_n, random_state=1)\n",
    "nonviolent_k = resample(nonviolent_df,  replace=False, n_samples=target_n, random_state=1)\n",
    "\n",
    "balanced_test = pd.concat([violent_down, nonviolent_k]).sample(frac=1, random_state=1)  # shuffle\n",
    "\n",
    "# --- split back to X, y ---\n",
    "X_test = balanced_test[\"feature\"].to_frame()\n",
    "y_test = balanced_test[\"label\"]\n",
    "\n",
    "print(\"the entire test dataset:\")\n",
    "print(y_test_full.value_counts())  # original imbalanced test\n",
    "print(\"the resampled balanced test dataset:\")\n",
    "print(y_test.value_counts())   # now balanced 1:1\n",
    "\n",
    "\n",
    "#### why do we do this ? \n",
    "\n",
    "### IMPORTANT EXPLANATION\n",
    "\n",
    "# our test dataset as is is imbalanced 70-30, (violent-non violent respectively)\n",
    "# there is nothing in research to suggest that the natural priors resemble these prior distributions.\n",
    "# Hence validating models on an imbalanced violent dataset would artificially boost precision for identifying violent texts.\n",
    "# It is safer to validate detectors on a balanced dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e49d34-f748-4ab8-922b-06a18234f13f",
   "metadata": {},
   "source": [
    "### Preprocess embeddings as this is costly in gridsearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d40de7-3b52-4197-a054-55f1e93339b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_texts = X_train[\"feature\"].astype(str).tolist()\n",
    "test_texts  = X_test[\"feature\"].astype(str).tolist()\n",
    "test_texts_full  = X_test_full[\"feature\"].astype(str).tolist()\n",
    "\n",
    "# model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "# st = SentenceTransformer(model_name)\n",
    "\n",
    "# X_train_minilm = st.encode(train_texts, batch_size=64, show_progress_bar=True)\n",
    "# X_test_minilm  = st.encode(test_texts,  batch_size=64, show_progress_bar=True)\n",
    "# X_test_minilm_full  = st.encode(test_texts_full,  batch_size=64, show_progress_bar=True)\n",
    "\n",
    "# X_train_minilm = np.asarray(X_train_minilm, dtype=np.float32)\n",
    "# X_test_minilm  = np.asarray(X_test_minilm,  dtype=np.float32)\n",
    "# X_test_minilm_full = np.asarray(X_test_minilm_full,  dtype=np.float32)\n",
    "\n",
    "# print(X_train_minilm.shape, X_test_minilm.shape)\n",
    "\n",
    "# np.save(\"X_train_minilm.npy\", X_train_minilm)\n",
    "# np.save(\"X_test_minilm.npy\", X_test_minilm)\n",
    "# np.save(\"X_test_minilm_full.npy\", X_test_minilm_full)\n",
    "\n",
    "# model_name = \"sentence-transformers/LaBSE\"\n",
    "# st = SentenceTransformer(model_name)\n",
    "\n",
    "# X_train_labse = st.encode(train_texts, batch_size=32, show_progress_bar=True)\n",
    "# X_test_labse  = st.encode(test_texts,  batch_size=32, show_progress_bar=True)\n",
    "# X_test_labse_full = st.encode(test_texts_full, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# X_train_labse = np.asarray(X_train_labse, dtype=np.float32)\n",
    "# X_test_labse  = np.asarray(X_test_labse,  dtype=np.float32)\n",
    "# X_test_labse_full = np.asarray(X_test_labse_full, dtype=np.float32)\n",
    "\n",
    "# np.save(\"X_train_labse.npy\", X_train_labse)\n",
    "# np.save(\"X_test_labse.npy\", X_test_labse)\n",
    "# np.save(\"X_test_labse_full.npy\", X_test_labse_full)\n",
    "\n",
    "\n",
    "\n",
    "# ft_transformer = FastTextTransformer(ft_model=ft)\n",
    "\n",
    "# X_train_fast = ft_transformer.transform(X_train[\"feature\"].astype(str).to_numpy())\n",
    "# X_test_fast  = ft_transformer.transform(X_test[\"feature\"].astype(str).to_numpy())\n",
    "# X_test_fast_full = ft_transformer.transform(X_test_full[\"feature\"].astype(str).to_numpy())\n",
    "\n",
    "# X_train_fast = np.asarray(X_train_fast, dtype=np.float32)\n",
    "# X_test_fast  = np.asarray(X_test_fast,  dtype=np.float32)\n",
    "# X_test_fast_full = np.asarray(X_test_fast_full, dtype=np.float32)\n",
    "\n",
    "# np.save(\"X_train_fast.npy\", X_train_fast)\n",
    "# np.save(\"X_test_fast.npy\", X_test_fast)\n",
    "# np.save(\"X_test_fast_full.npy\", X_test_fast_full)\n",
    "\n",
    "X_train_labse = np.load(\"X_train_labse.npy\")\n",
    "X_test_labse  = np.load(\"X_test_labse.npy\")\n",
    "X_test_labse_full  = np.load(\"X_test_labse_full.npy\")\n",
    "\n",
    "X_train_minilm = np.load(\"X_train_minilm.npy\")\n",
    "X_test_minilm  = np.load(\"X_test_minilm.npy\")\n",
    "X_test_minilm_full = np.load(\"X_test_minilm_full.npy\")\n",
    "\n",
    "X_train_fast = np.load(\"X_train_fast.npy\")\n",
    "X_test_fast  = np.load(\"X_test_fast.npy\")\n",
    "X_test_fast_full  = np.load(\"X_test_fast_full.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94aaf8b-44b9-43f3-baef-a9c8196214b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd2b70-83f6-48d6-b83e-89bebba1e5a9",
   "metadata": {},
   "source": [
    "### Sklearn pipelines\n",
    "\n",
    "Below we define the sk learn pipelines which involve\n",
    "\n",
    "Define pipelines, \n",
    "Note df is full dataset and bdf is balanced dataset\n",
    "\n",
    "* defining preprocessors (tf-idf, embeddings, bag of words, etc)\n",
    "* Defining classifiers to train (type of model )\n",
    "* Defining scoring metrics\n",
    "* Defining hyperparameter tuning search grids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac81a2e0-6959-4ecb-9d3a-93d32b0779f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor_tfidf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tfidf\", TfidfVectorizer(), \"feature\")  # Apply TfidfVectorizer to the 'feature' column\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "classifierRF = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\",preprocessor_tfidf),\n",
    "        (\"rf\",RandomForestClassifier(n_estimators=100,random_state=0,criterion=\"gini\",class_weight=\"balanced\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifierSVC = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\",preprocessor_tfidf),\n",
    "        (\"svc\",svm.SVC(random_state=0,class_weight=\"balanced\",probability=True))\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifierLR = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\",preprocessor_tfidf),\n",
    "        (\"logreg\", LogisticRegression(solver=\"liblinear\", random_state=0,max_iter=5000,class_weight='balanced'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "classifierSVC_emb = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  ##### normalizing the sparse embeddings space \n",
    "        (\"svc\",svm.SVC(random_state=0,kernel=\"linear\",class_weight=\"balanced\",probability=True))\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifierLR_emb = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),   # normalizing the embeddings space. \n",
    "        (\"logreg\", LogisticRegression(solver=\"liblinear\", random_state=0,max_iter=5000,class_weight='balanced'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "param_grid_svc = {\n",
    "    \"svc__C\": [.1,1, 10, 100],\n",
    "    \"svc__kernel\": ['linear', 'rbf', 'sigmoid'],\n",
    "    \"svc__gamma\": [\"auto\",\"scale\"],\n",
    "    # \"svc__class_weight\":[None, 'balanced']\n",
    "}\n",
    "\n",
    "param_grid_svc_emb = {\n",
    "    \"svc__C\": [.1,1, 5, 10],\n",
    "    # \"svc__kernel\": ['linear', 'rbf', 'sigmoid'],  RMEMBER WHY THIS IS BAD IDEA. CHECK ON GPT WHY ITS BAD FOR TEXT. ony linear good\n",
    "    # \"svc__gamma\": [\"auto\",\"scale\"], \n",
    "    # \"svc__class_weight\":[None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_rf = {\n",
    "    'rf__n_estimators': [100, 200, 500],\n",
    "    'rf__max_depth': [None, 10, 20, 30],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    'rf__max_features': ['sqrt', 'log2'],\n",
    "    'rf__bootstrap': [True, False],\n",
    "    'rf__criterion': ['gini', 'entropy'],\n",
    "    # 'rf__class_weight':[None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_logreg = {\n",
    "    'logreg__C': [0.01, 0.1, 1, 10],\n",
    "    'logreg__penalty': ['l1', 'l2'],\n",
    "    'logreg__solver': ['liblinear','saga'],\n",
    "    # 'logreg__class_weight':[None, 'balanced']\n",
    "}\n",
    "\n",
    "# please note that given the imblanace in the training data, we use class weight balanced so we penalize more erverley the errors in the\n",
    "# minority class, so our classifier is not biased towards the majority class which is violent.  \n",
    "\n",
    "\n",
    "\n",
    "# scorer = make_scorer(accuracy_score)\n",
    "scorer = make_scorer(precision_score,pos_label=VIOLENT_LABEL,average=\"binary\",zero_division=0.0)\n",
    "\n",
    "# we optimize for precision because a false positive is more costly than a false negative. \n",
    "\n",
    "\n",
    "# best_svc = \\\n",
    "# GridSearchCV(classifierSVC, param_grid_svc,n_jobs=8,cv=5,\n",
    "#              scoring=scorer\n",
    "# )\n",
    "# best_rf = GridSearchCV(classifierRF, param_grid_rf,n_jobs=8,cv=5,\n",
    "#                        scoring=scorer\n",
    "#                         )\n",
    "# best_logreg = GridSearchCV(classifierLR, param_grid_logreg,n_jobs=8,cv=5,scoring=scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3229b9e-d11f-4586-bb70-5ffa49caed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = {\n",
    "    'model_svc_tfidf':(classifierSVC,param_grid_svc),\n",
    "    'model_rf_tfidf': (classifierRF,param_grid_rf),\n",
    "    'model_logreg_tfidf':(classifierLR, param_grid_logreg),\n",
    "    \n",
    "    'r_model_svc_emb_fast':(classifierSVC_emb,param_grid_svc_emb, X_train_fast, y_train),\n",
    "    'r_model_logreg_emb_fast':(classifierLR_emb, param_grid_logreg, X_train_fast, y_train),\n",
    "    \n",
    "    'r_model_svc_emb_minilm':(classifierSVC_emb,param_grid_svc_emb, X_train_minilm, y_train),\n",
    "    'r_model_logreg_emb_minilm':(classifierLR_emb, param_grid_logreg, X_train_minilm, y_train),\n",
    "\n",
    "    'r_model_svc_emb_labse':(classifierSVC_emb,param_grid_svc_emb, X_train_labse, y_train),\n",
    "    'r_model_logreg_emb_labse':(classifierLR_emb, param_grid_logreg, X_train_labse, y_train),\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d64b1-06d5-43c5-a890-d0b772bb296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_models = {}\n",
    "\n",
    "# for model_name,model_logic in models_to_train.items():\n",
    "#     classifier, param_grid = model_logic\n",
    "#     best_model = GridSearchCV(classifier, param_grid,n_jobs=8,cv=5,scoring=scorer)\n",
    "#     best_model.fit(X_train, y_train)\n",
    "#     trained_models[model_name] = best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17306265-eb52-4b7a-8ede-ee1e8e4672be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Starting: r_model_svc_emb_fast\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished: r_model_svc_emb_fast in 12m 58s\n",
      "   Best CV score: 0.9553\n",
      "   Best params: {'svc__C': 0.1}\n",
      "\n",
      "ðŸ”„ Starting: r_model_logreg_emb_fast\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "âœ… Finished: r_model_logreg_emb_fast in 2m 19s\n",
      "   Best CV score: 0.9644\n",
      "   Best params: {'logreg__C': 0.01, 'logreg__penalty': 'l2', 'logreg__solver': 'liblinear'}\n",
      "\n",
      "ðŸ”„ Starting: r_model_svc_emb_minilm\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] END .........................................svc__C=0.1; total time=  17.3s\n",
      "[CV] END ...........................................svc__C=1; total time=  34.1s\n",
      "[CV] END ...........................................svc__C=1; total time=  30.4s\n",
      "[CV] END ...........................................svc__C=5; total time= 2.4min\n",
      "[CV] END ..........................................svc__C=10; total time= 4.9min\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l1, logreg__solver=liblinear; total time=   0.1s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l1, logreg__solver=saga; total time=   1.2s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l2, logreg__solver=liblinear; total time=   0.3s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l2, logreg__solver=liblinear; total time=   0.3s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l2, logreg__solver=saga; total time=   1.1s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l2, logreg__solver=saga; total time=   1.2s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l1, logreg__solver=saga; total time=  10.6s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l1, logreg__solver=saga; total time=   9.0s\n",
      "[CV] END logreg__C=1, logreg__penalty=l1, logreg__solver=liblinear; total time=   3.4s\n",
      "[CV] END logreg__C=1, logreg__penalty=l1, logreg__solver=saga; total time=  24.3s\n",
      "[CV] END logreg__C=1, logreg__penalty=l2, logreg__solver=liblinear; total time=   0.8s\n",
      "[CV] END logreg__C=1, logreg__penalty=l2, logreg__solver=saga; total time=  24.2s\n",
      "[CV] END logreg__C=10, logreg__penalty=l1, logreg__solver=liblinear; total time=   5.3s\n",
      "[CV] END logreg__C=10, logreg__penalty=l1, logreg__solver=liblinear; total time=   4.3s\n",
      "[CV] END logreg__C=10, logreg__penalty=l1, logreg__solver=saga; total time=  26.4s\n",
      "[CV] END logreg__C=10, logreg__penalty=l2, logreg__solver=liblinear; total time=   0.9s\n",
      "[CV] END logreg__C=10, logreg__penalty=l2, logreg__solver=saga; total time=  12.7s\n",
      "[CV] END logreg__C=10, logreg__penalty=l2, logreg__solver=saga; total time=  11.4s\n",
      "[CV] END .........................................svc__C=0.1; total time=  20.8s\n",
      "[CV] END ...........................................svc__C=1; total time=  42.4s\n",
      "[CV] END ...........................................svc__C=1; total time=  39.3s\n",
      "[CV] END ...........................................svc__C=5; total time= 2.9min\n",
      "[CV] END ..........................................svc__C=10; total time= 6.9min\n",
      "[CV] END .........................................svc__C=0.1; total time=  17.1s\n",
      "[CV] END ...........................................svc__C=1; total time=  36.1s\n",
      "[CV] END ...........................................svc__C=5; total time= 2.5min\n",
      "[CV] END ..........................................svc__C=10; total time= 4.8min\n",
      "[CV] END ..........................................svc__C=10; total time= 4.7min\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l1, logreg__solver=liblinear; total time=   0.1s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l1, logreg__solver=saga; total time=   1.1s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l2, logreg__solver=liblinear; total time=   0.2s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l2, logreg__solver=liblinear; total time=   0.3s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l2, logreg__solver=saga; total time=   1.1s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l2, logreg__solver=saga; total time=   1.2s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l1, logreg__solver=liblinear; total time=   0.8s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l1, logreg__solver=saga; total time=  10.9s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l2, logreg__solver=saga; total time=   6.0s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l2, logreg__solver=saga; total time=   5.2s\n",
      "[CV] END logreg__C=1, logreg__penalty=l1, logreg__solver=liblinear; total time=   3.2s\n",
      "[CV] END logreg__C=1, logreg__penalty=l1, logreg__solver=saga; total time=  22.3s\n",
      "[CV] END logreg__C=1, logreg__penalty=l2, logreg__solver=saga; total time=  22.5s\n",
      "[CV] END logreg__C=10, logreg__penalty=l1, logreg__solver=liblinear; total time=   4.7s\n",
      "[CV] END logreg__C=10, logreg__penalty=l1, logreg__solver=liblinear; total time=   4.7s\n",
      "[CV] END logreg__C=10, logreg__penalty=l1, logreg__solver=saga; total time=  25.5s\n",
      "[CV] END logreg__C=10, logreg__penalty=l1, logreg__solver=saga; total time=  23.6s\n",
      "[CV] END .........................................svc__C=0.1; total time=  20.4s\n",
      "[CV] END ...........................................svc__C=1; total time=  50.3s\n",
      "[CV] END ...........................................svc__C=5; total time= 3.3min\n",
      "[CV] END ..........................................svc__C=10; total time= 7.8min\n",
      "âœ… Finished: r_model_svc_emb_minilm in 19m 41s\n",
      "   Best CV score: 0.9529\n",
      "   Best params: {'svc__C': 0.1}\n",
      "\n",
      "ðŸ”„ Starting: r_model_logreg_emb_minilm\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished: r_model_logreg_emb_minilm in 1m 9s\n",
      "   Best CV score: 0.9607\n",
      "   Best params: {'logreg__C': 0.01, 'logreg__penalty': 'l2', 'logreg__solver': 'liblinear'}\n",
      "\n",
      "ðŸ”„ Starting: r_model_svc_emb_labse\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "âœ… Finished: r_model_svc_emb_labse in 6m 17s\n",
      "   Best CV score: 0.9554\n",
      "   Best params: {'svc__C': 0.1}\n",
      "\n",
      "ðŸ”„ Starting: r_model_logreg_emb_labse\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l1, logreg__solver=saga; total time=   0.6s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l2, logreg__solver=saga; total time=   0.5s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l2, logreg__solver=saga; total time=   0.6s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l1, logreg__solver=saga; total time=   2.8s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l1, logreg__solver=saga; total time=   2.5s\n",
      "[CV] END logreg__C=1, logreg__penalty=l1, logreg__solver=liblinear; total time=   1.7s\n",
      "[CV] END logreg__C=1, logreg__penalty=l1, logreg__solver=liblinear; total time=   1.8s\n",
      "[CV] END logreg__C=1, logreg__penalty=l1, logreg__solver=saga; total time=   8.6s\n",
      "[CV] END logreg__C=1, logreg__penalty=l2, logreg__solver=liblinear; total time=   1.0s\n",
      "[CV] END logreg__C=1, logreg__penalty=l2, logreg__solver=saga; total time=   6.5s\n",
      "[CV] END logreg__C=1, logreg__penalty=l2, logreg__solver=saga; total time=   6.4s\n",
      "[CV] END logreg__C=10, logreg__penalty=l1, logreg__solver=liblinear; total time=   3.7s\n",
      "[CV] END logreg__C=10, logreg__penalty=l1, logreg__solver=saga; total time=  15.4s\n",
      "[CV] END logreg__C=10, logreg__penalty=l2, logreg__solver=saga; total time=   7.6s\n",
      "[CV] END logreg__C=10, logreg__penalty=l2, logreg__solver=saga; total time=   8.0s\n",
      "[CV] END .........................................svc__C=0.1; total time=  28.1s\n",
      "[CV] END ...........................................svc__C=1; total time=  40.5s\n",
      "[CV] END ...........................................svc__C=1; total time=  36.5s\n",
      "[CV] END ...........................................svc__C=5; total time= 1.2min\n",
      "[CV] END ..........................................svc__C=10; total time= 1.4min\n",
      "[CV] END ..........................................svc__C=10; total time= 1.4min\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l1, logreg__solver=liblinear; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END logreg__C=0.01, logreg__penalty=l1, logreg__solver=liblinear; total time=   0.1s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l2, logreg__solver=liblinear; total time=   0.3s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l2, logreg__solver=liblinear; total time=   0.3s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l2, logreg__solver=saga; total time=   0.5s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l2, logreg__solver=saga; total time=   0.5s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l1, logreg__solver=saga; total time=   2.8s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l1, logreg__solver=saga; total time=   2.2s\n",
      "[CV] END logreg__C=1, logreg__penalty=l1, logreg__solver=liblinear; total time=   1.7s\n",
      "[CV] END logreg__C=1, logreg__penalty=l1, logreg__solver=liblinear; total time=   1.6s\n",
      "[CV] END logreg__C=1, logreg__penalty=l1, logreg__solver=saga; total time=   8.0s\n",
      "[CV] END logreg__C=1, logreg__penalty=l1, logreg__solver=saga; total time=   9.0s\n",
      "[CV] END logreg__C=1, logreg__penalty=l2, logreg__solver=saga; total time=   6.4s\n",
      "[CV] END logreg__C=10, logreg__penalty=l1, logreg__solver=saga; total time=  15.7s\n",
      "[CV] END logreg__C=10, logreg__penalty=l2, logreg__solver=liblinear; total time=   1.0s\n",
      "[CV] END logreg__C=10, logreg__penalty=l2, logreg__solver=liblinear; total time=   1.1s\n",
      "[CV] END logreg__C=10, logreg__penalty=l2, logreg__solver=liblinear; total time=   1.1s\n",
      "[CV] END logreg__C=10, logreg__penalty=l2, logreg__solver=saga; total time=   8.0s\n",
      "[CV] END .........................................svc__C=0.1; total time=  28.6s\n",
      "[CV] END ...........................................svc__C=1; total time=  43.5s\n",
      "[CV] END ...........................................svc__C=5; total time= 1.7min\n",
      "[CV] END ..........................................svc__C=10; total time= 1.5min\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l1, logreg__solver=liblinear; total time=   0.2s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l1, logreg__solver=liblinear; total time=   0.2s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l1, logreg__solver=saga; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END logreg__C=0.01, logreg__penalty=l1, logreg__solver=saga; total time=   1.1s\n",
      "[CV] END logreg__C=0.01, logreg__penalty=l1, logreg__solver=saga; total time=   1.1s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l1, logreg__solver=liblinear; total time=   0.5s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l1, logreg__solver=liblinear; total time=   0.5s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l1, logreg__solver=liblinear; total time=   0.5s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l1, logreg__solver=saga; total time=   5.5s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l2, logreg__solver=liblinear; total time=   1.2s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l2, logreg__solver=liblinear; total time=   1.2s\n",
      "[CV] END logreg__C=0.1, logreg__penalty=l2, logreg__solver=saga; total time=   4.1s\n",
      "[CV] END logreg__C=1, logreg__penalty=l1, logreg__solver=liblinear; total time=   3.5s\n",
      "[CV] END logreg__C=1, logreg__penalty=l1, logreg__solver=liblinear; total time=   3.4s\n",
      "[CV] END logreg__C=1, logreg__penalty=l1, logreg__solver=saga; total time=  26.5s\n",
      "[CV] END logreg__C=1, logreg__penalty=l2, logreg__solver=liblinear; total time=   2.0s\n",
      "[CV] END logreg__C=1, logreg__penalty=l2, logreg__solver=liblinear; total time=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/fbahena/Desktop/growth/masters/nlp_course/ia_hate/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished: r_model_logreg_emb_labse in 5m 22s\n",
      "   Best CV score: 0.9695\n",
      "   Best params: {'logreg__C': 0.01, 'logreg__penalty': 'l2', 'logreg__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "for model_name, (classifier, param_grid, X_tr, y_tr) in models_to_train.items():\n",
    "    \n",
    "\n",
    "    print(f\"\\nðŸ”„ Starting: {model_name}\")\n",
    "    start = time.time()\n",
    "\n",
    "    best_model = GridSearchCV(\n",
    "        classifier,\n",
    "        param_grid,\n",
    "        n_jobs=4,\n",
    "        cv=5,\n",
    "        scoring=scorer,\n",
    "        verbose=2,          # shows progress inside the grid\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    best_model.fit(X_tr, y_tr)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    mins = int(elapsed // 60)\n",
    "    secs = int(elapsed % 60)\n",
    "\n",
    "    print(f\"âœ… Finished: {model_name} in {mins}m {secs}s\")\n",
    "    print(f\"   Best CV score: {best_model.best_score_:.4f}\")\n",
    "    print(f\"   Best params: {best_model.best_params_}\")\n",
    "\n",
    "    trained_models[model_name] = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68310872-2a31-46dd-bfe4-a94523bbbbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7806188c-ff9b-4edb-b043-0c5d83d52e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "for model_name,model in trained_models.items():\n",
    "    filename = f'{model_name}.pkl'\n",
    "\n",
    "    #save model\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "# Load the model\n",
    "# with open(filename, 'rb') as file:\n",
    "#     loaded_model = pickle.load(file)\n",
    "\n",
    "# # Use the loaded model\n",
    "# predictions = loaded_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2bf54-c704-4086-abb3-8a86de0f8460",
   "metadata": {},
   "source": [
    "#### List models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4e40eda-5b92-4499-8422-fb7047edf19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### MODEL: model_logreg_tfidf\n",
      "\n",
      "Best params were :{'logreg__C': 1, 'logreg__penalty': 'l1', 'logreg__solver': 'liblinear'}\n",
      "\n",
      "The model model_logreg_tfidf metrics: \n",
      "\n",
      "Precision: 0.91, Recall: 0.82 \n",
      "\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.84      0.92      0.88       913\n",
      "     violent       0.91      0.82      0.86       913\n",
      "\n",
      "    accuracy                           0.87      1826\n",
      "   macro avg       0.87      0.87      0.87      1826\n",
      "weighted avg       0.87      0.87      0.87      1826\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#### MODEL: r_model_logreg_emb_fast\n",
      "\n",
      "Best params were :{'logreg__C': 0.01, 'logreg__penalty': 'l2', 'logreg__solver': 'liblinear'}\n",
      "\n",
      "The model r_model_logreg_emb_fast metrics: \n",
      "\n",
      "Precision: 0.91, Recall: 0.87 \n",
      "\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.87      0.92      0.90       913\n",
      "     violent       0.91      0.87      0.89       913\n",
      "\n",
      "    accuracy                           0.89      1826\n",
      "   macro avg       0.89      0.89      0.89      1826\n",
      "weighted avg       0.89      0.89      0.89      1826\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#### MODEL: r_model_svc_emb_fast\n",
      "\n",
      "Best params were :{'svc__C': 0.1}\n",
      "\n",
      "The model r_model_svc_emb_fast metrics: \n",
      "\n",
      "Precision: 0.89, Recall: 0.90 \n",
      "\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.90      0.89      0.90       913\n",
      "     violent       0.89      0.90      0.90       913\n",
      "\n",
      "    accuracy                           0.90      1826\n",
      "   macro avg       0.90      0.90      0.90      1826\n",
      "weighted avg       0.90      0.90      0.90      1826\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#### MODEL: r_model_svc_emb_labse\n",
      "\n",
      "Best params were :{'svc__C': 0.1}\n",
      "\n",
      "The model r_model_svc_emb_labse metrics: \n",
      "\n",
      "Precision: 0.90, Recall: 0.93 \n",
      "\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.93      0.89      0.91       913\n",
      "     violent       0.90      0.93      0.91       913\n",
      "\n",
      "    accuracy                           0.91      1826\n",
      "   macro avg       0.91      0.91      0.91      1826\n",
      "weighted avg       0.91      0.91      0.91      1826\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#### MODEL: model_rf_tfidf\n",
      "\n",
      "Best params were :{'rf__bootstrap': False, 'rf__class_weight': 'balanced', 'rf__criterion': 'entropy', 'rf__max_depth': 10, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 4, 'rf__min_samples_split': 10, 'rf__n_estimators': 500}\n",
      "\n",
      "The model model_rf_tfidf metrics: \n",
      "\n",
      "Precision: 0.94, Recall: 0.69 \n",
      "\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.76      0.96      0.84       913\n",
      "     violent       0.94      0.69      0.80       913\n",
      "\n",
      "    accuracy                           0.82      1826\n",
      "   macro avg       0.85      0.82      0.82      1826\n",
      "weighted avg       0.85      0.82      0.82      1826\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#### MODEL: r_model_logreg_emb_minilm\n",
      "\n",
      "Best params were :{'logreg__C': 0.01, 'logreg__penalty': 'l2', 'logreg__solver': 'liblinear'}\n",
      "\n",
      "The model r_model_logreg_emb_minilm metrics: \n",
      "\n",
      "Precision: 0.91, Recall: 0.88 \n",
      "\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.89      0.92      0.90       913\n",
      "     violent       0.91      0.88      0.90       913\n",
      "\n",
      "    accuracy                           0.90      1826\n",
      "   macro avg       0.90      0.90      0.90      1826\n",
      "weighted avg       0.90      0.90      0.90      1826\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#### MODEL: model_svc_tfidf\n",
      "\n",
      "Best params were :{'svc__C': 1, 'svc__class_weight': 'balanced', 'svc__gamma': 'scale', 'svc__kernel': 'sigmoid'}\n",
      "\n",
      "The model model_svc_tfidf metrics: \n",
      "\n",
      "Precision: 0.88, Recall: 0.91 \n",
      "\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.90      0.88      0.89       913\n",
      "     violent       0.88      0.91      0.89       913\n",
      "\n",
      "    accuracy                           0.89      1826\n",
      "   macro avg       0.89      0.89      0.89      1826\n",
      "weighted avg       0.89      0.89      0.89      1826\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#### MODEL: r_model_svc_emb_minilm\n",
      "\n",
      "Best params were :{'svc__C': 0.1}\n",
      "\n",
      "The model r_model_svc_emb_minilm metrics: \n",
      "\n",
      "Precision: 0.90, Recall: 0.89 \n",
      "\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.89      0.90      0.90       913\n",
      "     violent       0.90      0.89      0.90       913\n",
      "\n",
      "    accuracy                           0.90      1826\n",
      "   macro avg       0.90      0.90      0.90      1826\n",
      "weighted avg       0.90      0.90      0.90      1826\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#### MODEL: r_model_logreg_emb_labse\n",
      "\n",
      "Best params were :{'logreg__C': 0.01, 'logreg__penalty': 'l2', 'logreg__solver': 'liblinear'}\n",
      "\n",
      "The model r_model_logreg_emb_labse metrics: \n",
      "\n",
      "Precision: 0.93, Recall: 0.89 \n",
      "\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.90      0.94      0.92       913\n",
      "     violent       0.93      0.89      0.91       913\n",
      "\n",
      "    accuracy                           0.91      1826\n",
      "   macro avg       0.91      0.91      0.91      1826\n",
      "weighted avg       0.91      0.91      0.91      1826\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_X_test(model_name, full=False):\n",
    "    \"\"\"\n",
    "    Returns the correct test matrix depending on:\n",
    "    - model_name (tfidf / fast / minilm / labse)\n",
    "    - whether full test set is requested\n",
    "    \"\"\"\n",
    "\n",
    "    if \"minilm\" in model_name:\n",
    "        return X_test_minilm_full if full else X_test_minilm\n",
    "\n",
    "    if \"labse\" in model_name:\n",
    "        return X_test_labse_full if full else X_test_labse\n",
    "\n",
    "    if \"fast\" in model_name:\n",
    "        return X_test_fast_full if full else X_test_fast\n",
    "\n",
    "    # Default: TF-IDF models use raw dataframe\n",
    "    return X_test_full if full else X_test\n",
    "\n",
    "model_files = [\n",
    "    f for f in os.listdir(\".\")\n",
    "    if f.endswith(\"pkl\") and os.path.isfile(f)\n",
    "]\n",
    "\n",
    "for model_file in model_files:\n",
    "    \n",
    "    model_name = model_file.replace(\".pkl\",\"\")\n",
    "    print(f\"\\n#### MODEL: {model_name}\")\n",
    "\n",
    "    with open(model_file, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "        print(f\"\\nBest params were :{model.best_params_}\")\n",
    "\n",
    "    #checking performance on training data\n",
    "    \n",
    "    # y_pred_model_train = model.predict(X_train)\n",
    "    # report_model_train = classification_report(y_train, y_pred_model_train)\n",
    "    \n",
    "    # checking performance on prod data.\n",
    "\n",
    "    #TODO HOW DO YOU CONSIDER TEST_BALANCED_DATASET FOR BELOW ? \n",
    "\n",
    "    balanced_test_dataset = True\n",
    "    \n",
    "    test_features = get_X_test(model_name, full=(not balanced_test_dataset))\n",
    "    test_labels = y_test if balanced_test_dataset else y_test_full\n",
    "\n",
    "\n",
    "\n",
    "    y_pred_model = model.predict(test_features)\n",
    "    report_model = classification_report(test_labels, y_pred_model)\n",
    "    report_dict = classification_report(test_labels, y_pred_model, output_dict=True)\n",
    "    print(f\"\\nThe model {model_name} metrics: \\n\")\n",
    "    print(f\"Precision: {report_dict['violent']['precision']:.2f}, Recall: {report_dict['violent']['recall']:.2f} \\n\")\n",
    "    print(\"the model report: \\n\")\n",
    "    print(report_model)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb6f3b-aa64-4958-97fc-dd758e03a4fe",
   "metadata": {},
   "source": [
    "### Discussion on Model rankings and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cefaf8f-fb11-498d-b18d-9fee51f3ae11",
   "metadata": {},
   "source": [
    "1. labse embeddings preprocessing for both svc and logreg seem to be the best models.\n",
    "2. Dataset is imbalanced, there is no prior info that suggests prior distributions in reality. Balancing the training dataset would have made the models incur in information-signal loss. Nonetheless by using the parameter `class_weight=balanced` we are able to adjust loss with weighting, instead of undersampling/information loss.\n",
    "3. For the evaluation dataset, the imablanced test dataset boosts precision more. Model rankings stay the same though if using balanced evaluation dataset, but precision is lower. Recall holds. So...labse is indeed the best preprocessing.\n",
    "4. We should threshold tuned the probabilities for both top wo models, the labse ones, and see which one boosts more precision without punishing recall a lot.\n",
    "\n",
    "Notes:\n",
    "In your report:\n",
    "\n",
    "Present balanced test metrics as primary comparison.\n",
    "\n",
    "Present imbalanced metrics as operational view.\n",
    "\n",
    "Emphasize ranking stability.\n",
    "\n",
    "Because your ranking stability is a very strong result.\n",
    "\n",
    "GOTCHAS: \n",
    "\n",
    "1. WHY LARGE C WAS BAD IN SVC WITH EMBEDDINGS\n",
    "\n",
    "In linear SVM, we optimize:\n",
    "\n",
    "1\n",
    "2\n",
    "âˆ£\n",
    "âˆ£\n",
    "ð‘¤\n",
    "âˆ£\n",
    "âˆ£\n",
    "2\n",
    "+\n",
    "ð¶\n",
    "âˆ‘\n",
    "ðœ‰\n",
    "ð‘–\n",
    "2\n",
    "1\n",
    "\tâ€‹\n",
    "\n",
    "âˆ£âˆ£wâˆ£âˆ£\n",
    "2\n",
    "+Câˆ‘Î¾\n",
    "i\n",
    "\tâ€‹\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "âˆ£\n",
    "âˆ£\n",
    "ð‘¤\n",
    "âˆ£\n",
    "âˆ£\n",
    "2\n",
    "âˆ£âˆ£wâˆ£âˆ£\n",
    "2\n",
    " controls margin size (regularization)\n",
    "\n",
    "ð¶\n",
    "C controls penalty for misclassification\n",
    "\n",
    "Small C â†’ stronger regularization â†’ larger margin â†’ smoother boundary â†’ faster training\n",
    "Large C â†’ weaker regularization â†’ tighter margin â†’ tries to fit all points â†’ slower convergence\n",
    "\n",
    "In high-dimensional text embeddings (e.g., FastText, MiniLM, LaBSE), classes are often nearly linearly separable. Therefore, small C values (0.1â€“1) usually generalize better and train much faster, while large C (10, 100) increases training time dramatically with little or no gain.\n",
    "\n",
    "2. WHY IT MADE NO SENSE TO TRY OTHER KERNELS IN SVC WITH EMBEDDINGS\n",
    "\n",
    "Why we use linear SVM (and not RBF/sigmoid) for text embeddings:\n",
    "\n",
    "Embeddings (FastText, MiniLM, LaBSE) are already nonlinear transformations learned by deep models. They map text into a semantically structured space.\n",
    "\n",
    "In this high-dimensional space, classes are often nearly linearly separable. A linear decision boundary is usually sufficient.\n",
    "\n",
    "Nonlinear kernels (RBF, sigmoid):\n",
    "\n",
    "Increase computational cost significantly (O(nÂ²) or worse)\n",
    "\n",
    "Risk overfitting\n",
    "\n",
    "Rarely improve performance for NLP embeddings\n",
    "\n",
    "Therefore, LinearSVC or LogisticRegression is typically faster, more stable, and just as effective for text classification with embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f3c611-f7c5-4e8d-944f-ecd0828875e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
