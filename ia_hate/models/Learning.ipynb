{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b09d198-80a3-4943-90f7-eea55956f0bc",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa7168b-71d5-4127-b080-18d954088eb0",
   "metadata": {},
   "source": [
    "## 1. Loading Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce86a5e-b5da-40d0-9136-b77fe4be3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics \\\n",
    "import classification_report, recall_score, accuracy_score,precision_score, make_scorer,confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle\n",
    "\n",
    "def myConfMatrix(confusion_array,labels,columns):\n",
    "    df = pd.DataFrame(confusion_array, index=labels, columns=columns)\n",
    "    return df\n",
    "\n",
    "def balanceDF(df):\n",
    "    violent = df.loc[df[\"label\"] == \"violent\"]\n",
    "    nonviolent = df.loc[df[\"label\"] == \"nonviolent\"]\n",
    "    violent_patched = violent.sample(nonviolent.shape[0],random_state=0)\n",
    "    #balanced\n",
    "    bdf = pd.concat([violent_patched,nonviolent])\n",
    "    return bdf\n",
    "\n",
    "### embeddings dependencies \n",
    "\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "ft = load_facebook_vectors(\"cc.es.300.bin\")\n",
    "\n",
    "_token_pat = re.compile(r\"[A-Za-zÃÃ‰ÃÃ“ÃšÃœÃ‘Ã¡Ã©Ã­Ã³ÃºÃ¼Ã±']+\")\n",
    "\n",
    "def _sentence_vec_fasttext(s, ft_model):\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if s is None else str(s)\n",
    "    toks = _token_pat.findall(s.lower())\n",
    "    vecs = [ft_model[w] for w in toks if w in ft_model.key_to_index]\n",
    "    return np.mean(vecs, axis=0).astype(np.float32) if vecs else np.zeros(ft_model.vector_size, dtype=np.float32)\n",
    "\n",
    "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ft_model, vector_size=None):\n",
    "        self.ft_model = ft_model\n",
    "        self.vector_size = vector_size  # optional override\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        texts = np.asarray(X).ravel()\n",
    "        vs = self.vector_size or self.ft_model.vector_size\n",
    "        # ensure consistent dtype/shape\n",
    "        out = np.zeros((len(texts), vs), dtype=np.float32)\n",
    "        for i, s in enumerate(texts):\n",
    "            out[i] = _sentence_vec_fasttext(s, self.ft_model)\n",
    "        return out\n",
    "\n",
    "###############\n",
    "\n",
    "\n",
    "VIOLENT_LABEL = \"violent\"\n",
    "NONVIOLENT_LABEL = \"nonviolent\"\n",
    "\n",
    "cm_labels = ['violent', 'nonviolent']\n",
    "cm_columns = ['Predicted violent', 'predicted nonviolent']\n",
    "\n",
    "\n",
    "\n",
    "categorical_features = [\"feature\"]\n",
    "\n",
    "# These were discarded as non informative. \n",
    "# numberic featuers will be ignored as they are non-informative, as can be seen on EDA.\n",
    "numeric_features = [\"length\", \"punct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54961e4f-54ff-481f-9ade-f825e586dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d8ce47-f666-4d17-a08a-5ff78e58ee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# class SentenceTransformerEncoder(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "#                  batch_size=64, normalize_embeddings=False, show_progress_bar=False):\n",
    "#         self.model_name = model_name\n",
    "#         self.batch_size = batch_size\n",
    "#         self.normalize_embeddings = normalize_embeddings\n",
    "#         self.show_progress_bar = show_progress_bar\n",
    "#         self._model = None\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         if self._model is None:\n",
    "#             self._model = SentenceTransformer(self.model_name)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         if self._model is None:\n",
    "#             self._model = SentenceTransformer(self.model_name)\n",
    "\n",
    "#         texts = np.asarray(X).ravel()\n",
    "#         texts = [\"\" if t is None else str(t) for t in texts]\n",
    "\n",
    "#         emb = self._model.encode(\n",
    "#             texts,\n",
    "#             batch_size=self.batch_size,\n",
    "#             show_progress_bar=self.show_progress_bar,\n",
    "#             normalize_embeddings=self.normalize_embeddings,\n",
    "#         )\n",
    "#         return np.asarray(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba745748-892f-4e43-bd9f-cdf1d1b9f062",
   "metadata": {},
   "source": [
    "### 1.1 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ce1d9-d42b-4707-aef3-6a6f46e8611f",
   "metadata": {},
   "source": [
    "* Map to Violent and Non-Violent tags. \n",
    "* Create total dataset and balanced dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09949439-fddc-46cd-8976-f0b75c67fde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full db data:\n",
      "label\n",
      "violent       8689\n",
      "nonviolent    3028\n",
      "Name: count, dtype: int64\n",
      "balanced db data:\n",
      "label\n",
      "violent       8689\n",
      "nonviolent    3028\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../master_data/data.csv\")\n",
    "# mapping label to 1 -> violent 0 -> non-violent\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: VIOLENT_LABEL if x == 1 else NONVIOLENT_LABEL)\n",
    "df['feature'] = df['feature'].str.replace('\\xa0', ' ', regex=False)\n",
    "\n",
    "bdf = balanceDF(df)\n",
    "\n",
    "print(\"full db data:\")\n",
    "print(df.label.value_counts())\n",
    "print(\"balanced db data:\")\n",
    "print(df.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63dbb6-3a1d-4b9a-bc8d-52f016c4ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f414065-fa7d-4d36-984d-cdde7a1bb17d",
   "metadata": {},
   "source": [
    "### Split dataset into train and test. \n",
    "\n",
    "Note that we rebalance the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e22072-bca2-41dc-aab2-3295d098ce3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the entire dataset is (11717, 4)\n",
      "the entire test dataset:\n",
      "label\n",
      "violent       2603\n",
      "nonviolent     913\n",
      "Name: count, dtype: int64\n",
      "the resampled balanced test dataset:\n",
      "label\n",
      "violent       913\n",
      "nonviolent    913\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "data = df\n",
    "X_train, X_test_full, y_train, y_test_full =\\\n",
    "train_test_split(data.loc[:,[\"feature\"]],data[\"label\"], test_size= 0.3, random_state= 1)\n",
    "print(f\"the shape of the entire dataset is {df.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- join X and y for resampling ---\n",
    "test_df = X_test_full.copy()\n",
    "test_df[\"label\"] = y_test_full.values\n",
    "\n",
    "# --- separate classes ---\n",
    "violent_df     = test_df[test_df[\"label\"] == \"violent\"]\n",
    "nonviolent_df  = test_df[test_df[\"label\"] == \"nonviolent\"]\n",
    "\n",
    "# target size = minority count (here nonviolent is smaller: 913)\n",
    "target_n = 913\n",
    "\n",
    "# --- downsample majority to match minority ---\n",
    "violent_down = resample(violent_df,     replace=False, n_samples=target_n, random_state=1)\n",
    "nonviolent_k = resample(nonviolent_df,  replace=False, n_samples=target_n, random_state=1)\n",
    "\n",
    "balanced_test = pd.concat([violent_down, nonviolent_k]).sample(frac=1, random_state=1)  # shuffle\n",
    "\n",
    "# --- split back to X, y ---\n",
    "X_test = balanced_test[\"feature\"].to_frame()\n",
    "y_test = balanced_test[\"label\"]\n",
    "\n",
    "print(\"the entire test dataset:\")\n",
    "print(y_test_full.value_counts())  # original imbalanced test\n",
    "print(\"the resampled balanced test dataset:\")\n",
    "print(y_test.value_counts())   # now balanced 1:1\n",
    "\n",
    "\n",
    "#### why do we do this ? \n",
    "\n",
    "### IMPORTANT EXPLANATION\n",
    "\n",
    "# our test dataset as is is imbalanced 70-30, (violent-non violent respectively)\n",
    "# there is nothing in research to suggest that the natural priors resemble these prior distributions.\n",
    "# Hence validating models on an imbalanced violent dataset would artificially boost precision for identifying violent texts.\n",
    "# It is safer to validate detectors on a balanced dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e49d34-f748-4ab8-922b-06a18234f13f",
   "metadata": {},
   "source": [
    "### Preprocess embeddings as this is costly in gridsearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4d40de7-3b52-4197-a054-55f1e93339b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d79b8f187f4fbea24fe7182f2075a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800b4730d18e4e53a963084ed1c89d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4d9f2b5fcb4109b508818b40631237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8201, 384) (1826, 384)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e43b1407b54de0bb667f854a7855fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4aff308fab49e1822d5426478888e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e02089e77a54148a624e7c7fad7699b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_texts = X_train[\"feature\"].astype(str).tolist()\n",
    "test_texts  = X_test[\"feature\"].astype(str).tolist()\n",
    "test_texts_full  = X_test_full[\"feature\"].astype(str).tolist()\n",
    "\n",
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "st = SentenceTransformer(model_name)\n",
    "\n",
    "X_train_minilm = st.encode(train_texts, batch_size=64, show_progress_bar=True)\n",
    "X_test_minilm  = st.encode(test_texts,  batch_size=64, show_progress_bar=True)\n",
    "X_test_minilm_full  = st.encode(test_texts_full,  batch_size=64, show_progress_bar=True)\n",
    "\n",
    "X_train_minilm = np.asarray(X_train_minilm, dtype=np.float32)\n",
    "X_test_minilm  = np.asarray(X_test_minilm,  dtype=np.float32)\n",
    "X_test_minilm_full = np.asarray(X_test_minilm_full,  dtype=np.float32)\n",
    "\n",
    "print(X_train_minilm.shape, X_test_minilm.shape)\n",
    "\n",
    "np.save(\"X_train_minilm.npy\", X_train_minilm)\n",
    "np.save(\"X_test_minilm.npy\", X_test_minilm)\n",
    "np.save(\"X_test_minilm_full.npy\", X_test_minilm_full)\n",
    "\n",
    "model_name = \"sentence-transformers/LaBSE\"\n",
    "st = SentenceTransformer(model_name)\n",
    "\n",
    "X_train_labse = st.encode(train_texts, batch_size=32, show_progress_bar=True)\n",
    "X_test_labse  = st.encode(test_texts,  batch_size=32, show_progress_bar=True)\n",
    "X_test_labse_full = st.encode(test_texts_full, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "X_train_labse = np.asarray(X_train_labse, dtype=np.float32)\n",
    "X_test_labse  = np.asarray(X_test_labse,  dtype=np.float32)\n",
    "X_test_labse_full = np.asarray(X_test_labse_full, dtype=np.float32)\n",
    "\n",
    "np.save(\"X_train_labse.npy\", X_train_labse)\n",
    "np.save(\"X_test_labse.npy\", X_test_labse)\n",
    "np.save(\"X_test_labse_full.npy\", X_test_labse_full)\n",
    "\n",
    "\n",
    "\n",
    "ft_transformer = FastTextTransformer(ft_model=ft)\n",
    "\n",
    "X_train_fast = ft_transformer.transform(X_train[\"feature\"].astype(str).to_numpy())\n",
    "X_test_fast  = ft_transformer.transform(X_test[\"feature\"].astype(str).to_numpy())\n",
    "X_test_fast_full = ft_transformer.transform(X_test_full[\"feature\"].astype(str).to_numpy())\n",
    "\n",
    "X_train_fast = np.asarray(X_train_fast, dtype=np.float32)\n",
    "X_test_fast  = np.asarray(X_test_fast,  dtype=np.float32)\n",
    "X_test_fast_full = np.asarray(X_test_fast_full, dtype=np.float32)\n",
    "\n",
    "np.save(\"X_train_fast.npy\", X_train_fast)\n",
    "np.save(\"X_test_fast.npy\", X_test_fast)\n",
    "np.save(\"X_test_fast_full.npy\", X_test_fast_full)\n",
    "\n",
    "X_train_labse = np.load(\"X_train_labse.npy\")\n",
    "X_test_labse  = np.load(\"X_test_labse.npy\")\n",
    "X_test_labse_full  = np.load(\"X_test_labse_full.npy\")\n",
    "\n",
    "X_train_minilm = np.load(\"X_train_minilm.npy\")\n",
    "X_test_minilm  = np.load(\"X_test_minilm.npy\")\n",
    "X_test_minilm_full = np.load(\"X_test_minilm_full.npy\")\n",
    "\n",
    "X_train_fast = np.load(\"X_train_fast.npy\")\n",
    "X_test_fast  = np.load(\"X_test_fast.npy\")\n",
    "X_test_fast_full  = np.load(\"X_test_fast_full.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94aaf8b-44b9-43f3-baef-a9c8196214b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd2b70-83f6-48d6-b83e-89bebba1e5a9",
   "metadata": {},
   "source": [
    "### Sklearn pipelines\n",
    "\n",
    "Below we define the sk learn pipelines which involve\n",
    "\n",
    "Define pipelines, \n",
    "Note df is full dataset and bdf is balanced dataset\n",
    "\n",
    "* defining preprocessors (tf-idf, embeddings, bag of words, etc)\n",
    "* Defining classifiers to train (type of model )\n",
    "* Defining scoring metrics\n",
    "* Defining hyperparameter tuning search grids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac81a2e0-6959-4ecb-9d3a-93d32b0779f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor_tfidf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tfidf\", TfidfVectorizer(), \"feature\")  # Apply TfidfVectorizer to the 'feature' column\n",
    "    ]\n",
    ")\n",
    "\n",
    "# preprocessor_fast_text = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"fasttext\", FastTextTransformer(), \"feature\")\n",
    "#     ],\n",
    "#    remainder=\"drop\"\n",
    "# )\n",
    "\n",
    "# preprocessor_minilm = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"minilm\", SentenceTransformerEncoder(\"paraphrase-multilingual-MiniLM-L12-v2\"), \"feature\")\n",
    "#     ],\n",
    "#     remainder=\"drop\"\n",
    "# )\n",
    "\n",
    "# preprocessor_labse = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"labse\", SentenceTransformerEncoder(\"sentence-transformers/LaBSE\"), \"feature\")\n",
    "#     ],\n",
    "#     remainder=\"drop\"\n",
    "# )\n",
    "\n",
    "# preprocessor_emb = preprocessor_minilm\n",
    "\n",
    "\n",
    "classifierRF = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\",preprocessor_tfidf),\n",
    "        (\"rf\",RandomForestClassifier(n_estimators=100,random_state=0,criterion=\"gini\",class_weight=\"balanced\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifierSVC = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\",preprocessor_tfidf),\n",
    "        (\"svc\",svm.SVC(random_state=0,class_weight=\"balanced\",probability=True))\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifierLR = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\",preprocessor_tfidf),\n",
    "        (\"logreg\", LogisticRegression(solver=\"liblinear\", random_state=0,max_iter=5000,class_weight='balanced'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "classifierSVC_emb = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  ##### normalizing the sparse embeddings space \n",
    "        (\"svc\",svm.SVC(random_state=0,kernel=\"linear\",class_weight=\"balanced\")) #,probability=True))\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifierLR_emb = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),   # normalizing the embeddings space. \n",
    "        (\"logreg\", LogisticRegression(solver=\"liblinear\", random_state=0,max_iter=5000,class_weight='balanced'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "param_grid_svc = {\n",
    "    \"svc__C\": [.1,1, 10, 100],\n",
    "    \"svc__kernel\": ['linear', 'rbf', 'sigmoid'],\n",
    "    \"svc__gamma\": [\"auto\",\"scale\"],\n",
    "    # \"svc__class_weight\":[None, 'balanced']\n",
    "}\n",
    "\n",
    "param_grid_svc_emb = {\n",
    "    \"svc__C\": [.1,1, 5, 10],\n",
    "    # \"svc__kernel\": ['linear', 'rbf', 'sigmoid'],  RMEMBER WHY THIS IS BAD IDEA. CHECK ON GPT WHY ITS BAD FOR TEXT. ony linear good\n",
    "    # \"svc__gamma\": [\"auto\",\"scale\"], \n",
    "    # \"svc__class_weight\":[None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_rf = {\n",
    "    'rf__n_estimators': [100, 200, 500],\n",
    "    'rf__max_depth': [None, 10, 20, 30],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    'rf__max_features': ['sqrt', 'log2'],\n",
    "    'rf__bootstrap': [True, False],\n",
    "    'rf__criterion': ['gini', 'entropy'],\n",
    "    # 'rf__class_weight':[None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_logreg = {\n",
    "    'logreg__C': [0.01, 0.1, 1, 10],\n",
    "    'logreg__penalty': ['l1', 'l2'],\n",
    "    'logreg__solver': ['liblinear','saga'],\n",
    "    # 'logreg__class_weight':[None, 'balanced']\n",
    "}\n",
    "\n",
    "# please note that given the imblanace in the training data, we use class weight balanced so we penalize more erverley the errors in the\n",
    "# minority class, so our classifier is not biased towards the majority class which is violent.  \n",
    "\n",
    "\n",
    "\n",
    "# scorer = make_scorer(accuracy_score)\n",
    "scorer = make_scorer(precision_score,pos_label=VIOLENT_LABEL,average=\"binary\",zero_division=0.0)\n",
    "\n",
    "# we optimize for precision because a false positive is more costly than a false negative. \n",
    "\n",
    "\n",
    "# best_svc = \\\n",
    "# GridSearchCV(classifierSVC, param_grid_svc,n_jobs=8,cv=5,\n",
    "#              scoring=scorer\n",
    "# )\n",
    "# best_rf = GridSearchCV(classifierRF, param_grid_rf,n_jobs=8,cv=5,\n",
    "#                        scoring=scorer\n",
    "#                         )\n",
    "# best_logreg = GridSearchCV(classifierLR, param_grid_logreg,n_jobs=8,cv=5,scoring=scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3229b9e-d11f-4586-bb70-5ffa49caed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = {\n",
    "    # 'model_svc':(classifierSVC,param_grid_svc),\n",
    "    # 'model_rf': (classifierRF,param_grid_rf),\n",
    "    # 'model_logreg':(classifierLR, param_grid_logreg)\n",
    "    \n",
    "    'r_model_svc_emb_fast':(classifierSVC_emb,param_grid_svc_emb, X_train_fast, y_train),\n",
    "    'r_model_logreg_emb_fast':(classifierLR_emb, param_grid_logreg, X_train_fast, y_train),\n",
    "    \n",
    "    'r_model_svc_emb_minilm':(classifierSVC_emb,param_grid_svc_emb, X_train_minilm, y_train),\n",
    "    'r_model_logreg_emb_minilm':(classifierLR_emb, param_grid_logreg, X_train_minilm, y_train),\n",
    "\n",
    "    'r_model_svc_emb_labse':(classifierSVC_emb,param_grid_svc_emb, X_train_labse, y_train),\n",
    "    'r_model_logreg_emb_labse':(classifierLR_emb, param_grid_logreg, X_train_labse, y_train),\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d64b1-06d5-43c5-a890-d0b772bb296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_models = {}\n",
    "\n",
    "# for model_name,model_logic in models_to_train.items():\n",
    "#     classifier, param_grid = model_logic\n",
    "#     best_model = GridSearchCV(classifier, param_grid,n_jobs=8,cv=5,scoring=scorer)\n",
    "#     best_model.fit(X_train, y_train)\n",
    "#     trained_models[model_name] = best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17306265-eb52-4b7a-8ede-ee1e8e4672be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "for model_name, (classifier, param_grid, X_tr, y_tr) in models_to_train.items():\n",
    "    \n",
    "\n",
    "    print(f\"\\nðŸ”„ Starting: {model_name}\")\n",
    "    start = time.time()\n",
    "\n",
    "    best_model = GridSearchCV(\n",
    "        classifier,\n",
    "        param_grid,\n",
    "        n_jobs=4,\n",
    "        cv=5,\n",
    "        scoring=scorer,\n",
    "        verbose=2,          # shows progress inside the grid\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    best_model.fit(X_tr, y_tr)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    mins = int(elapsed // 60)\n",
    "    secs = int(elapsed % 60)\n",
    "\n",
    "    print(f\"âœ… Finished: {model_name} in {mins}m {secs}s\")\n",
    "    print(f\"   Best CV score: {best_model.best_score_:.4f}\")\n",
    "    print(f\"   Best params: {best_model.best_params_}\")\n",
    "\n",
    "    trained_models[model_name] = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68310872-2a31-46dd-bfe4-a94523bbbbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7806188c-ff9b-4edb-b043-0c5d83d52e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "for model_name,model in trained_models.items():\n",
    "    filename = f'{model_name}.pkl'\n",
    "\n",
    "    #save model\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "# Load the model\n",
    "# with open(filename, 'rb') as file:\n",
    "#     loaded_model = pickle.load(file)\n",
    "\n",
    "# # Use the loaded model\n",
    "# predictions = loaded_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2bf54-c704-4086-abb3-8a86de0f8460",
   "metadata": {},
   "source": [
    "#### List models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4e40eda-5b92-4499-8422-fb7047edf19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### MODEL: r_model_logreg_emb_fast\n",
      "Best params were :{'logreg__C': 0.01, 'logreg__penalty': 'l2', 'logreg__solver': 'liblinear'}\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.87      0.92      0.90       913\n",
      "     violent       0.91      0.87      0.89       913\n",
      "\n",
      "    accuracy                           0.89      1826\n",
      "   macro avg       0.89      0.89      0.89      1826\n",
      "weighted avg       0.89      0.89      0.89      1826\n",
      "\n",
      "\n",
      "\n",
      "#### MODEL: r_model_svc_emb_fast\n",
      "Best params were :{'svc__C': 0.1}\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.90      0.89      0.90       913\n",
      "     violent       0.89      0.90      0.90       913\n",
      "\n",
      "    accuracy                           0.90      1826\n",
      "   macro avg       0.90      0.90      0.90      1826\n",
      "weighted avg       0.90      0.90      0.90      1826\n",
      "\n",
      "\n",
      "\n",
      "#### MODEL: r_model_svc_emb_labse\n",
      "Best params were :{'svc__C': 0.1}\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.93      0.89      0.91       913\n",
      "     violent       0.90      0.93      0.91       913\n",
      "\n",
      "    accuracy                           0.91      1826\n",
      "   macro avg       0.91      0.91      0.91      1826\n",
      "weighted avg       0.91      0.91      0.91      1826\n",
      "\n",
      "\n",
      "\n",
      "#### MODEL: r_model_logreg_emb_minilm\n",
      "Best params were :{'logreg__C': 0.01, 'logreg__penalty': 'l2', 'logreg__solver': 'liblinear'}\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.89      0.92      0.90       913\n",
      "     violent       0.91      0.88      0.90       913\n",
      "\n",
      "    accuracy                           0.90      1826\n",
      "   macro avg       0.90      0.90      0.90      1826\n",
      "weighted avg       0.90      0.90      0.90      1826\n",
      "\n",
      "\n",
      "\n",
      "#### MODEL: r_model_svc_emb_minilm\n",
      "Best params were :{'svc__C': 0.1}\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.89      0.90      0.90       913\n",
      "     violent       0.90      0.89      0.90       913\n",
      "\n",
      "    accuracy                           0.90      1826\n",
      "   macro avg       0.90      0.90      0.90      1826\n",
      "weighted avg       0.90      0.90      0.90      1826\n",
      "\n",
      "\n",
      "\n",
      "#### MODEL: r_model_logreg_emb_labse\n",
      "Best params were :{'logreg__C': 0.01, 'logreg__penalty': 'l2', 'logreg__solver': 'liblinear'}\n",
      "the model report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  nonviolent       0.90      0.94      0.92       913\n",
      "     violent       0.93      0.89      0.91       913\n",
      "\n",
      "    accuracy                           0.91      1826\n",
      "   macro avg       0.91      0.91      0.91      1826\n",
      "weighted avg       0.91      0.91      0.91      1826\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_X_test(model_name, full=False):\n",
    "    \"\"\"\n",
    "    Returns the correct test matrix depending on:\n",
    "    - model_name (tfidf / fast / minilm / labse)\n",
    "    - whether full test set is requested\n",
    "    \"\"\"\n",
    "\n",
    "    if \"minilm\" in model_name:\n",
    "        return X_test_minilm_full if full else X_test_minilm\n",
    "\n",
    "    if \"labse\" in model_name:\n",
    "        return X_test_labse_full if full else X_test_labse\n",
    "\n",
    "    if \"fast\" in model_name:\n",
    "        return X_test_fast_full if full else X_test_fast\n",
    "\n",
    "    # Default: TF-IDF models use raw dataframe\n",
    "    return X_test_full if full else X_test\n",
    "\n",
    "model_files = [\n",
    "    f for f in os.listdir(\".\")\n",
    "    if f.startswith(\"r_model_\") and os.path.isfile(f)\n",
    "]\n",
    "\n",
    "for model_file in model_files:\n",
    "    \n",
    "    model_name = model_file.replace(\".pkl\",\"\")\n",
    "    print(f\"#### MODEL: {model_name}\")\n",
    "\n",
    "    with open(model_file, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "        print(f\"Best params were :{model.best_params_}\")\n",
    "\n",
    "    #checking performance on training data\n",
    "    \n",
    "    # y_pred_model_train = model.predict(X_train)\n",
    "    # report_model_train = classification_report(y_train, y_pred_model_train)\n",
    "    \n",
    "    # checking performance on prod data.\n",
    "\n",
    "    #TODO HOW DO YOU CONSIDER TEST_BALANCED_DATASET FOR BELOW ? \n",
    "\n",
    "    # if \"minilm\" in model_name:\n",
    "    #     test_dataset = X_test_minilm_full\n",
    "    # elif \"labse\" in model_name:\n",
    "    #     test_dataset = X_test_labse_full\n",
    "    # elif \"fast\" in model_name:\n",
    "    #     test_dataset = X_test_fast_full\n",
    "    # else:\n",
    "    #     test_dataset = X_test_full# tfidf models\n",
    "\n",
    "    balanced_test_dataset = True\n",
    "    \n",
    "    test_features = get_X_test(model_name, full=(not balanced_test_dataset))\n",
    "    test_labels = y_test if balanced_test_dataset else y_test_full\n",
    "\n",
    "\n",
    "\n",
    "    y_pred_model = model.predict(test_features)\n",
    "    report_model = classification_report(test_labels, y_pred_model)\n",
    "    print(\"the model report: \\n\")\n",
    "    print(report_model)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "\n",
    "# for model_name,_ in models_to_train.items():\n",
    "#     print(f\"#######MODEL: {model_name}\")\n",
    "#     with open(model_name+f\"_{preprocess_type}.pkl\", 'rb') as file:\n",
    "#         model = pickle.load(file)\n",
    "#     print(f\"Best parameters to evaluate on (CV score={model.best_score_:.3f}):\")\n",
    "#     print(f\"Best params were :{model.best_params_}\")\n",
    "\n",
    "#     #checking performance on training data\n",
    "    \n",
    "#     # y_pred_model_train = model.predict(X_train)\n",
    "#     # report_model_train = classification_report(y_train, y_pred_model_train)\n",
    "    \n",
    "#     # checking performance on prod data.\n",
    "#     y_pred_model = model.predict(X_test_full)\n",
    "#     report_model = classification_report(y_test_full, y_pred_model)\n",
    "#     print(\"the model report: \\n\")\n",
    "#     print(report_model)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b24be-aa96-4b62-875d-9baea027e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8cb42-a1f8-4772-9eac-e280ba4e3814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
